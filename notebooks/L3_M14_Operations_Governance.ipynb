{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3 M14.3: Tenant Lifecycle & Migrations\n",
    "\n",
    "## Learning Arc\n",
    "\n",
    "**Purpose:** Master zero-downtime tenant migrations, GDPR-compliant data deletion workflows, and backup/restore orchestration for enterprise multi-tenant RAG systems. This module teaches production-grade lifecycle management patterns used at Global Capability Centers (GCCs) serving Fortune 500 clients.\n",
    "\n",
    "**Concepts Covered:**\n",
    "- Blue-green deployment patterns with gradual traffic cutover (10% → 25% → 50% → 100%)\n",
    "- GDPR Article 17 deletion workflows across 7+ systems (PostgreSQL, Redis, Pinecone, S3, CloudWatch, backups, analytics)\n",
    "- Per-tenant backup and restore with point-in-time recovery\n",
    "- Tenant cloning for staging/testing with data anonymization\n",
    "- Sub-second rollback capability for failed migrations\n",
    "- Multi-system orchestration using Celery and APScheduler\n",
    "- Data consistency verification using checksums\n",
    "- Legal hold checks preventing accidental deletion during litigation\n",
    "- Cryptographically signed deletion certificates for compliance audit trails\n",
    "- Cost-benefit analysis: zero-downtime (₹40-60 lakh) vs maintenance window (₹2-5 lakh)\n",
    "\n",
    "**After Completing This Notebook:**\n",
    "- You will understand how to design blue-green migrations with dual-write mode and incremental sync\n",
    "- You can implement GDPR deletion workflows that systematically erase data across all systems\n",
    "- You will recognize when to use zero-downtime migration vs maintenance windows vs rolling updates\n",
    "- You can build backup/restore systems with configurable retention and cross-region replication\n",
    "- You will handle common failure scenarios: data inconsistency, rollback failures, incomplete deletion\n",
    "- You can debug migrations using verification scans, checksums, and deletion certificates\n",
    "- You will implement legal hold checks to prevent regulatory violations\n",
    "- You can orchestrate multi-system operations with parallel workers and distributed locks\n",
    "\n",
    "**Context in Track L3.M14:**\n",
    "This module builds on **M14.1 (Multi-Tenant Architecture)** and **M14.2 (Tenant Isolation & Security)** by adding lifecycle management: onboarding, migration, offboarding, and disaster recovery. It prepares you for **M14.4 (Operating Model & Governance)** where you'll learn cost allocation, SLA management, and incident response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path for imports\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.insert(0, '../src')\n",
    "\n",
    "# OFFLINE mode for L3 consistency (no external infrastructure required)\n",
    "OFFLINE = os.getenv(\"OFFLINE\", \"true\").lower() == \"true\"\n",
    "\n",
    "# Infrastructure services (optional - disabled by default)\n",
    "POSTGRES_ENABLED = os.getenv(\"POSTGRES_ENABLED\", \"false\").lower() == \"true\"\n",
    "REDIS_ENABLED = os.getenv(\"REDIS_ENABLED\", \"false\").lower() == \"true\"\n",
    "PINECONE_ENABLED = os.getenv(\"PINECONE_ENABLED\", \"false\").lower() == \"true\"\n",
    "AWS_ENABLED = os.getenv(\"AWS_ENABLED\", \"false\").lower() == \"true\"\n",
    "\n",
    "print(\"L3 M14.3: Tenant Lifecycle & Migrations\")\n",
    "print(\"=\"*50)\n",
    "print(f\"OFFLINE mode: {OFFLINE}\")\n",
    "print(f\"PostgreSQL: {'enabled' if POSTGRES_ENABLED else 'disabled'}\")\n",
    "print(f\"Redis: {'enabled' if REDIS_ENABLED else 'disabled'}\")\n",
    "print(f\"Pinecone: {'enabled' if PINECONE_ENABLED else 'disabled'}\")\n",
    "print(f\"AWS S3: {'enabled' if AWS_ENABLED else 'disabled'}\")\n",
    "print()\n",
    "\n",
    "if OFFLINE:\n",
    "    print(\"⚠️  Running in OFFLINE mode\")\n",
    "    print(\"   → All infrastructure calls will be simulated\")\n",
    "    print(\"   → Set OFFLINE=false in .env to enable real operations\")\n",
    "else:\n",
    "    print(\"✓ Online mode - infrastructure operations enabled\")\n",
    "    if not any([POSTGRES_ENABLED, REDIS_ENABLED, PINECONE_ENABLED, AWS_ENABLED]):\n",
    "        print(\"⚠️  Warning: Online mode but no services enabled\")\n",
    "        print(\"   → Enable services in .env (POSTGRES_ENABLED=true, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Blue-Green Migration Fundamentals\n",
    "\n",
    "Blue-green deployment is a zero-downtime migration strategy where you:\n",
    "1. Run parallel **blue** (current) and **green** (new) environments\n",
    "2. Synchronize data between them\n",
    "3. Gradually shift traffic from blue to green\n",
    "4. Keep blue as instant rollback option\n",
    "\n",
    "**Why zero-downtime matters:**\n",
    "- Platinum tier SLAs guarantee 99.99% uptime (max 52 minutes downtime/year)\n",
    "- Financial services regulations prohibit trading hour outages\n",
    "- Revenue impact: ₹1-5 lakh/hour for high-volume tenants\n",
    "- Customer trust: Single outage can trigger contract renegotiation\n",
    "\n",
    "**Six-Phase Migration:**\n",
    "1. **Provision:** Spin up green infrastructure (Terraform, K8s)\n",
    "2. **Full Sync:** Bulk data transfer (AWS DataSync, pg_dump)\n",
    "3. **Dual-Write:** Application writes to both blue and green\n",
    "4. **Incremental Sync:** Close replication gap from transaction logs\n",
    "5. **Cutover:** Gradual traffic shift (10% → 25% → 50% → 100%)\n",
    "6. **Decommission:** Destroy blue after 24-hour stability period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import migration functions\n",
    "from l3_m14_tenant_lifecycle import migrate_tenant_blue_green, MigrationStatus\n",
    "\n",
    "# Example: Migrate a Platinum tier tenant\n",
    "tenant_id = \"tenant_platinum_001\"\n",
    "source_env = \"blue\"\n",
    "target_env = \"green\"\n",
    "\n",
    "print(f\"Initiating blue-green migration: {tenant_id}\")\n",
    "print(f\"Source: {source_env} → Target: {target_env}\")\n",
    "print()\n",
    "\n",
    "result = migrate_tenant_blue_green(\n",
    "    tenant_id=tenant_id,\n",
    "    source_env=source_env,\n",
    "    target_env=target_env,\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Migration result: {result['status']}\")\n",
    "if result.get('skipped'):\n",
    "    print(f\"Reason: {result['reason']}\")\n",
    "else:\n",
    "    print(f\"Duration: {result.get('duration_seconds', 'N/A')} seconds\")\n",
    "    print(f\"Traffic percentage: {result.get('traffic_percentage', 'N/A')}%\")\n",
    "\n",
    "# Expected (offline mode): Skipped with simulation message\n",
    "# Expected (online mode): Full 6-phase migration with checksum validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: GDPR Article 17 Deletion Workflow\n",
    "\n",
    "GDPR Article 17 grants EU citizens the **\"right to erasure\"** (right to be forgotten). When a tenant requests deletion:\n",
    "- You have **30 days** to complete erasure across ALL systems\n",
    "- Missing even one system risks **€20 million** fine or 4% global revenue\n",
    "- Must handle **legal holds** (court orders preventing deletion)\n",
    "- Must provide **cryptographically signed certificate** as proof\n",
    "\n",
    "**Seven Systems to Delete:**\n",
    "1. **PostgreSQL:** Tenant registry, metadata, user accounts\n",
    "2. **Redis:** Session cache, distributed locks, rate limit counters\n",
    "3. **Pinecone:** Vector embeddings, semantic search indices\n",
    "4. **S3:** Documents, images, file uploads, backups\n",
    "5. **CloudWatch:** Application logs (anonymize tenant_id references)\n",
    "6. **Backups:** Add to exclusion list, schedule purge\n",
    "7. **Analytics:** Event streams, dashboards, aggregated metrics\n",
    "\n",
    "**Production Reality:**\n",
    "- First attempts find residual data in ~30% of verifications\n",
    "- Common miss: Analytics databases not in deletion checklist\n",
    "- Common miss: Backup tapes stored offsite for 7 years\n",
    "- Manual audit required for first 5 deletions to build confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GDPR deletion functions\n",
    "from l3_m14_tenant_lifecycle import execute_gdpr_deletion, verify_gdpr_deletion, generate_deletion_certificate\n",
    "import uuid\n",
    "\n",
    "# Example: Execute GDPR deletion for offboarded tenant\n",
    "tenant_id = \"tenant_offboarded_001\"\n",
    "request_id = str(uuid.uuid4())\n",
    "\n",
    "print(f\"GDPR Deletion Request\")\n",
    "print(f\"Tenant: {tenant_id}\")\n",
    "print(f\"Request ID: {request_id}\")\n",
    "print()\n",
    "\n",
    "deletion_result = execute_gdpr_deletion(\n",
    "    tenant_id=tenant_id,\n",
    "    request_id=request_id,\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Deletion status: {deletion_result['status']}\")\n",
    "if deletion_result.get('skipped'):\n",
    "    print(f\"Reason: {deletion_result['reason']}\")\n",
    "else:\n",
    "    print(f\"Systems deleted: {', '.join(deletion_result['systems_deleted'])}\")\n",
    "    print(f\"Certificate ID: {deletion_result['certificate_id']}\")\n",
    "    print(f\"Completed at: {deletion_result['completed_at']}\")\n",
    "\n",
    "# Expected (offline): Simulated deletion across all systems\n",
    "# Expected (online): Parallel deletion + verification + certificate generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Deletion Verification and Certificate\n",
    "\n",
    "**Why verification matters:**\n",
    "- GDPR audits require proof of complete erasure\n",
    "- Residual data in any system violates compliance\n",
    "- Certificates provide cryptographic non-repudiation\n",
    "\n",
    "**Verification Process:**\n",
    "1. Query each system for tenant_id presence\n",
    "2. Aggregate results into pass/fail per system\n",
    "3. Generate deletion report\n",
    "4. If complete: Sign certificate with SHA-256\n",
    "5. If incomplete: Block certificate, flag for manual review\n",
    "\n",
    "**Certificate Contents:**\n",
    "- Certificate ID (unique identifier)\n",
    "- Tenant ID and deletion request ID\n",
    "- Timestamp of deletion\n",
    "- List of systems verified\n",
    "- Verification status (complete/incomplete)\n",
    "- SHA-256 signature for tamper-proof audit trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GDPR deletion completeness\n",
    "systems_to_verify = [\"postgresql\", \"redis\", \"pinecone\", \"s3\", \"cloudwatch\", \"backups\", \"analytics\"]\n",
    "\n",
    "print(f\"Verifying deletion for tenant: {tenant_id}\")\n",
    "print(f\"Systems to check: {len(systems_to_verify)}\")\n",
    "print()\n",
    "\n",
    "verification = verify_gdpr_deletion(\n",
    "    tenant_id=tenant_id,\n",
    "    systems=systems_to_verify,\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Verification complete: {verification['complete']}\")\n",
    "print(f\"Systems checked: {len(verification['systems_checked'])}\")\n",
    "print()\n",
    "print(\"Per-system status:\")\n",
    "for system, status in verification['system_status'].items():\n",
    "    status_icon = \"✓\" if status else \"✗\"\n",
    "    print(f\"  {status_icon} {system}: {'deleted' if status else 'residual data found'}\")\n",
    "\n",
    "if verification['complete']:\n",
    "    print(\"\\n✓ All systems verified clean - generating certificate...\")\n",
    "    certificate = generate_deletion_certificate(\n",
    "        tenant_id=tenant_id,\n",
    "        request_id=request_id,\n",
    "        verification=verification,\n",
    "        offline=OFFLINE\n",
    "    )\n",
    "    print(f\"Certificate ID: {certificate['certificate_id']}\")\n",
    "    print(f\"Signature: {certificate['signature'][:16]}...\")\n",
    "else:\n",
    "    print(\"\\n✗ Verification incomplete - manual review required\")\n",
    "    print(f\"Remaining data in: {verification['remaining_data']}\")\n",
    "\n",
    "# Expected: All systems show deleted in offline mode\n",
    "# In production: May find residual data in first attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Backup and Restore with Point-in-Time Recovery\n",
    "\n",
    "**Backup strategies by tenant tier:**\n",
    "- **Platinum:** Daily backups, 365 days retention, cross-region replication\n",
    "- **Gold:** Weekly backups, 90 days retention, single region\n",
    "- **Bronze:** Monthly backups, 30 days retention, single region\n",
    "\n",
    "**Point-in-Time Recovery (PITR):**\n",
    "- Restore to any timestamp within retention window\n",
    "- Uses transaction logs + base backup\n",
    "- Critical for ransomware recovery, data corruption, accidental deletion\n",
    "\n",
    "**Cross-Region Replication:**\n",
    "- Protects against regional disasters (datacenter fire, network partition)\n",
    "- Adds 50-100ms replication lag\n",
    "- Doubles storage cost but ensures business continuity\n",
    "\n",
    "**Systems Backed Up:**\n",
    "1. PostgreSQL: Full pg_dump + WAL logs\n",
    "2. Redis: RDB snapshots + AOF logs\n",
    "3. Pinecone: Namespace export to S3\n",
    "4. S3: Versioning + cross-region replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import backup/restore functions\n",
    "from l3_m14_tenant_lifecycle import create_tenant_backup, restore_tenant_backup\n",
    "\n",
    "# Example: Create backup for Platinum tier tenant\n",
    "tenant_id = \"tenant_platinum_002\"\n",
    "\n",
    "print(f\"Creating backup for tenant: {tenant_id}\")\n",
    "print(f\"Tier: Platinum (365 days retention, cross-region)\")\n",
    "print()\n",
    "\n",
    "backup_result = create_tenant_backup(\n",
    "    tenant_id=tenant_id,\n",
    "    retention_days=365,\n",
    "    cross_region=True,\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Backup status: {backup_result['status']}\")\n",
    "if backup_result.get('skipped'):\n",
    "    print(f\"Reason: {backup_result['reason']}\")\n",
    "else:\n",
    "    print(f\"Backup ID: {backup_result['backup_id']}\")\n",
    "    print(f\"Size: {backup_result['size_bytes'] / (1024*1024):.2f} MB\")\n",
    "    print(f\"Cross-region: {backup_result['cross_region']}\")\n",
    "    print(f\"Systems backed up: {', '.join(backup_result['systems'].keys())}\")\n",
    "\n",
    "# Save backup_id for restore demo\n",
    "backup_id = backup_result.get('backup_id', 'backup_demo_001')\n",
    "\n",
    "# Expected: Backup created across 4 systems (PostgreSQL, Redis, Pinecone, S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Restore from backup (disaster recovery scenario)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Scenario: Restore to 24 hours ago (before data corruption)\n",
    "point_in_time = datetime.now() - timedelta(hours=24)\n",
    "\n",
    "print(f\"Restoring tenant: {tenant_id}\")\n",
    "print(f\"From backup: {backup_id}\")\n",
    "print(f\"Point-in-time: {point_in_time.isoformat()}\")\n",
    "print()\n",
    "\n",
    "restore_result = restore_tenant_backup(\n",
    "    backup_id=backup_id,\n",
    "    tenant_id=tenant_id,\n",
    "    point_in_time=point_in_time,\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Restore status: {restore_result['status']}\")\n",
    "if restore_result.get('skipped'):\n",
    "    print(f\"Reason: {restore_result['reason']}\")\n",
    "else:\n",
    "    print(f\"Systems restored: {', '.join(restore_result['systems_restored'])}\")\n",
    "    print(f\"Point-in-time: {restore_result['point_in_time']}\")\n",
    "    print(f\"Verification: {restore_result['verification']['success']}\")\n",
    "\n",
    "# Expected: Schema compatibility check + restore + verification\n",
    "# Production: 15-60 minutes depending on data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Tenant Cloning for Staging and Testing\n",
    "\n",
    "**Why clone tenants:**\n",
    "- Test new features in staging before production rollout\n",
    "- Reproduce production bugs in safe environment\n",
    "- Create development environments with realistic data\n",
    "- Validate migration workflows before live cutover\n",
    "\n",
    "**Data Anonymization (Critical):**\n",
    "- **PII masking:** Replace names with fake data (Faker library)\n",
    "- **Email obfuscation:** user@example.com → user_12345@testdomain.com\n",
    "- **Tokenization:** Replace credit cards with test numbers\n",
    "- **Aggregation:** Sum revenue instead of itemized transactions\n",
    "\n",
    "**Selective Synchronization:**\n",
    "- **Full clone:** All data types (documents, embeddings, metadata, configs)\n",
    "- **Config-only:** Just settings, no user data\n",
    "- **Schema-only:** Database structure, no records\n",
    "- **Metadata + configs:** Lightweight testing environment\n",
    "\n",
    "**Legal Consideration:**\n",
    "- GDPR prohibits copying PII to non-production without anonymization\n",
    "- HIPAA requires same security controls in staging as production\n",
    "- Always anonymize by default unless explicit legal approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clone function\n",
    "from l3_m14_tenant_lifecycle import clone_tenant\n",
    "\n",
    "# Example: Clone production tenant to staging with anonymization\n",
    "source_tenant = \"tenant_production_001\"\n",
    "target_tenant = \"tenant_staging_001\"\n",
    "\n",
    "print(f\"Cloning tenant for staging environment\")\n",
    "print(f\"Source: {source_tenant} (production)\")\n",
    "print(f\"Target: {target_tenant} (staging)\")\n",
    "print(f\"Anonymization: ENABLED (GDPR compliance)\")\n",
    "print()\n",
    "\n",
    "clone_result = clone_tenant(\n",
    "    source_tenant_id=source_tenant,\n",
    "    target_tenant_id=target_tenant,\n",
    "    anonymize_data=True,  # Always True for staging\n",
    "    selective_sync=[\"documents\", \"embeddings\", \"configs\"],  # Exclude raw user data\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Clone status: {clone_result['status']}\")\n",
    "if clone_result.get('skipped'):\n",
    "    print(f\"Reason: {clone_result['reason']}\")\n",
    "else:\n",
    "    print(f\"Data types cloned: {', '.join(clone_result['data_types'])}\")\n",
    "    print(f\"Anonymized: {clone_result['anonymized']}\")\n",
    "    print(f\"Timestamp: {clone_result['timestamp']}\")\n",
    "\n",
    "# Expected: Selective clone with PII anonymization\n",
    "# Use staging environment to test new features safely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Migration Rollback and Data Consistency\n",
    "\n",
    "**Why rollback matters:**\n",
    "- 15-20% of first migrations require rollback\n",
    "- Target rollback time: **<60 seconds** from detection to traffic recovery\n",
    "- Faster rollback = less customer impact = preserved SLA\n",
    "\n",
    "**Rollback Triggers:**\n",
    "1. Data consistency failure (checksum mismatch)\n",
    "2. Target environment errors (500s spike >5%)\n",
    "3. Latency regression (p99 latency >2x baseline)\n",
    "4. Business metric drop (conversion rate <80% baseline)\n",
    "5. Manual trigger (engineering judgment)\n",
    "\n",
    "**Rollback Process:**\n",
    "1. **Immediate:** Route 100% traffic back to blue (DNS/load balancer update)\n",
    "2. **Verify:** Confirm traffic flowing to blue, error rate normalized\n",
    "3. **Restore:** Apply rollback snapshot if data corruption detected\n",
    "4. **Post-mortem:** Document failure reason, update playbook\n",
    "\n",
    "**Data Consistency Verification:**\n",
    "- Calculate MD5/SHA checksums for each system\n",
    "- Compare source vs target after sync\n",
    "- Report differences by system and data type\n",
    "- Block cutover if consistency <100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import verification and rollback functions\n",
    "from l3_m14_tenant_lifecycle import verify_data_consistency, rollback_migration\n",
    "\n",
    "# Example: Verify data consistency before cutover\n",
    "tenant_id = \"tenant_migration_001\"\n",
    "source_env = \"blue\"\n",
    "target_env = \"green\"\n",
    "\n",
    "print(f\"Verifying data consistency for tenant: {tenant_id}\")\n",
    "print(f\"Source: {source_env} vs Target: {target_env}\")\n",
    "print()\n",
    "\n",
    "consistency = verify_data_consistency(\n",
    "    tenant_id=tenant_id,\n",
    "    source_env=source_env,\n",
    "    target_env=target_env,\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Consistency check: {'PASS' if consistency['consistent'] else 'FAIL'}\")\n",
    "print(f\"Systems checked: {', '.join(consistency['systems_checked'])}\")\n",
    "\n",
    "if consistency['consistent']:\n",
    "    print(\"\\n✓ All checksums match - safe to proceed with cutover\")\n",
    "else:\n",
    "    print(f\"\\n✗ Data inconsistency detected in {len(consistency['differences'])} systems\")\n",
    "    print(\"Differences:\")\n",
    "    for diff in consistency['differences']:\n",
    "        print(f\"  - {diff['system']}: {diff['source_checksum']} != {diff['target_checksum']}\")\n",
    "    print(\"\\nAction: Trigger full re-sync before proceeding\")\n",
    "\n",
    "# Expected (offline): Consistent (simulated checksums match)\n",
    "# Production: May detect mismatches requiring re-sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Rollback failed migration\n",
    "# Scenario: Green environment showing high error rate\n",
    "\n",
    "rollback_snapshot = \"backup_pre_migration_001\"\n",
    "\n",
    "print(f\"ROLLBACK INITIATED\")\n",
    "print(f\"Tenant: {tenant_id}\")\n",
    "print(f\"Reason: Target environment error rate >5%\")\n",
    "print(f\"Rollback snapshot: {rollback_snapshot}\")\n",
    "print()\n",
    "\n",
    "rollback_result = rollback_migration(\n",
    "    tenant_id=tenant_id,\n",
    "    rollback_snapshot=rollback_snapshot,\n",
    "    offline=OFFLINE\n",
    ")\n",
    "\n",
    "print(f\"Rollback status: {rollback_result['status']}\")\n",
    "if rollback_result.get('skipped'):\n",
    "    print(f\"Reason: {rollback_result['reason']}\")\n",
    "else:\n",
    "    print(f\"Rollback duration: {rollback_result['duration_seconds']:.2f} seconds\")\n",
    "    print(f\"Target: <60 seconds (SLA requirement)\")\n",
    "    \n",
    "    if rollback_result['duration_seconds'] < 60:\n",
    "        print(\"\\n✓ Rollback completed within SLA\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Rollback exceeded SLA - review DNS/load balancer config\")\n",
    "\n",
    "# Expected: Sub-second simulated rollback\n",
    "# Production target: <60 seconds from decision to traffic recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Common Failures and Debugging\n",
    "\n",
    "Based on production experience from GCCs serving Fortune 500 clients:\n",
    "\n",
    "### Failure 1: Data Inconsistency (Checksum Mismatch)\n",
    "**Symptom:** Verification shows different checksums between source and target\n",
    "**Cause:** High write rate outpaces incremental sync, network partition during sync\n",
    "**Fix:** Full re-sync with parallel workers, increase sync frequency from 5min to 1min\n",
    "\n",
    "### Failure 2: Rollback Failure (Load Balancer Config Drift)\n",
    "**Symptom:** DNS updated but traffic still hitting green environment\n",
    "**Cause:** Load balancer configuration out of sync with Terraform state\n",
    "**Fix:** Manual DNS update to blue IP, verify health checks enabled, quarterly rollback drills\n",
    "\n",
    "### Failure 3: Incomplete GDPR Deletion\n",
    "**Symptom:** Verification finds residual data in analytics database\n",
    "**Cause:** Analytics system not documented in deletion checklist\n",
    "**Fix:** Multi-system discovery scan (`grep -r tenant_id`), update deletion checklist, manual audit for first 5 deletions\n",
    "\n",
    "### Failure 4: Migration Timeout (High Write Rate)\n",
    "**Symptom:** Incremental sync never catches up, replication lag >30 minutes\n",
    "**Cause:** Tenant writing >10K records/second, single sync worker saturated\n",
    "**Fix:** Scale parallel workers from 4 to 16, consider short maintenance window for write-heavy tenants\n",
    "\n",
    "### Failure 5: Backup Restoration Failure (Schema Incompatibility)\n",
    "**Symptom:** Restore fails with column mismatch error\n",
    "**Cause:** Production schema evolved (added columns) but backup from older version\n",
    "**Fix:** Schema migration before restore, maintain version metadata in backups, monthly restore testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging: Simulate common failure scenarios\n",
    "\n",
    "print(\"Common Failure Scenarios - Debugging Guide\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scenario 1: Data Inconsistency\n",
    "print(\"\\n1. DATA INCONSISTENCY\")\n",
    "print(\"Symptoms:\")\n",
    "print(\"  - Checksum mismatch in verification\")\n",
    "print(\"  - Different record counts between blue and green\")\n",
    "print(\"Debug commands:\")\n",
    "print(\"  verify_data_consistency(tenant_id, 'blue', 'green')\")\n",
    "print(\"  # Check each system's checksum individually\")\n",
    "print(\"Fix:\")\n",
    "print(\"  # Full re-sync with increased parallelism\")\n",
    "print(\"  # Extend sync window, reduce write rate during migration\")\n",
    "\n",
    "# Scenario 2: Rollback Failure\n",
    "print(\"\\n2. ROLLBACK FAILURE\")\n",
    "print(\"Symptoms:\")\n",
    "print(\"  - Traffic still hitting green despite DNS update\")\n",
    "print(\"  - Rollback duration >60 seconds\")\n",
    "print(\"Debug commands:\")\n",
    "print(\"  # Check load balancer config\")\n",
    "print(\"  # Verify DNS propagation: dig tenant.example.com\")\n",
    "print(\"  # Check health checks: curl http://blue-lb/health\")\n",
    "print(\"Fix:\")\n",
    "print(\"  # Manual DNS update to blue IP\")\n",
    "print(\"  # Quarterly rollback drills to catch config drift\")\n",
    "\n",
    "# Scenario 3: Incomplete GDPR Deletion\n",
    "print(\"\\n3. INCOMPLETE GDPR DELETION\")\n",
    "print(\"Symptoms:\")\n",
    "print(\"  - Verification finds residual data in 1+ systems\")\n",
    "print(\"  - Deletion certificate blocked\")\n",
    "print(\"Debug commands:\")\n",
    "print(\"  verify_gdpr_deletion(tenant_id, all_systems)\")\n",
    "print(\"  # Discovery scan: grep -r tenant_id /var/log/* /backups/*\")\n",
    "print(\"Fix:\")\n",
    "print(\"  # Update deletion checklist with missing systems\")\n",
    "print(\"  # Re-run deletion workflow\")\n",
    "print(\"  # Manual audit for first 5 deletions\")\n",
    "\n",
    "# Scenario 4: Migration Timeout\n",
    "print(\"\\n4. MIGRATION TIMEOUT\")\n",
    "print(\"Symptoms:\")\n",
    "print(\"  - Incremental sync replication lag >30 minutes\")\n",
    "print(\"  - Never reaches 100% caught up\")\n",
    "print(\"Debug commands:\")\n",
    "print(\"  # Check write rate: SELECT COUNT(*) FROM writes WHERE timestamp > NOW() - INTERVAL '1 minute'\")\n",
    "print(\"  # Monitor sync workers: top -p <sync_worker_pid>\")\n",
    "print(\"Fix:\")\n",
    "print(\"  # Scale workers from 4 to 16\")\n",
    "print(\"  # Consider short maintenance window for write-heavy tenants\")\n",
    "\n",
    "# Scenario 5: Backup Restoration Failure\n",
    "print(\"\\n5. BACKUP RESTORATION FAILURE\")\n",
    "print(\"Symptoms:\")\n",
    "print(\"  - Restore fails with schema error (column mismatch)\")\n",
    "print(\"  - Version incompatibility between backup and current\")\n",
    "print(\"Debug commands:\")\n",
    "print(\"  # Check backup metadata: cat backup_manifest.json\")\n",
    "print(\"  # Compare schemas: pg_dump --schema-only current | diff - backup_schema.sql\")\n",
    "print(\"Fix:\")\n",
    "print(\"  # Run schema migration before restore\")\n",
    "print(\"  # Monthly restore testing to catch incompatibilities early\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Decision Card - When to Use Zero-Downtime Migration\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "| Approach | Development Cost | Operational Cost | Downtime | Best For |\n",
    "|----------|-----------------|------------------|----------|----------|\n",
    "| **Zero-Downtime** | ₹40-60 lakh | ₹50K-80K/migration | 0 minutes | Platinum tenants, 99.99% SLA |\n",
    "| **Maintenance Window** | ₹2-5 lakh | ₹5K-15K/migration | 2-4 hours | Gold tenants, scheduled updates |\n",
    "| **Rolling K8s Update** | Minimal | Minimal | 0 minutes | Code-only, no data migration |\n",
    "| **Hybrid Tiered** | ₹15-25 lakh | Varies by tier | Varies | Multi-tier customer base |\n",
    "\n",
    "### When to Use Zero-Downtime\n",
    "\n",
    "**Use this approach when:**\n",
    "- Tenant has strict SLA requiring 99.99% uptime (max 52 minutes/year)\n",
    "- Revenue impact >₹1 lakh/hour during downtime\n",
    "- Tenant tier is Platinum/Enterprise with premium support\n",
    "- Migration spans business hours or peak usage times\n",
    "- Regulatory compliance prohibits outages (financial trading, healthcare)\n",
    "- Previous migrations caused customer escalations or churn\n",
    "- Multi-region rollout with gradual geographic cutover\n",
    "- Testing new infrastructure version before full commitment (canary)\n",
    "\n",
    "**Do NOT use when:**\n",
    "- Tenant accepts scheduled maintenance windows (Bronze/Silver tiers)\n",
    "- Off-peak migration window available (nights, weekends for B2B)\n",
    "- Same-region updates compatible with Kubernetes rolling deployment\n",
    "- Budget constraints: ₹40-60 lakh development cost too high for ROI\n",
    "- Small tenant with <10K queries/day (backup-restore faster)\n",
    "- Infrastructure change is backward-compatible (no data migration)\n",
    "- First-time migration (20% rollback rate too risky)\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "- **Migration duration:** 6-8 hours (first attempt) → 3-4 hours (experienced)\n",
    "- **Rollback time:** <60 seconds (SLA requirement)\n",
    "- **Success rate:** 80% (first) → 98%+ (after 3-5 migrations)\n",
    "- **Data consistency:** 100% (checksum verification)\n",
    "- **Operational cost:** ₹50K-80K per migration (parallel infrastructure)\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Complexity:**\n",
    "- 5000+ lines of orchestration code\n",
    "- 7+ systems to coordinate\n",
    "- Senior DevOps/SRE expertise required\n",
    "- Quarterly rollback testing burden\n",
    "\n",
    "**Latency:**\n",
    "- +5-10ms write latency during dual-write mode\n",
    "- Gradual cutover extends total migration time (vs instant cutover)\n",
    "\n",
    "**Cost:**\n",
    "- 10-20x higher development cost vs maintenance window\n",
    "- 3-5x higher operational cost (parallel infrastructure)\n",
    "- But: 0 revenue loss from downtime, preserved customer trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Calculator: Zero-Downtime vs Maintenance Window\n",
    "\n",
    "def calculate_migration_costs(tenant_tier, queries_per_day, revenue_per_hour_downtime):\n",
    "    \"\"\"\n",
    "    Calculate total cost of ownership for different migration strategies.\n",
    "    \n",
    "    Args:\n",
    "        tenant_tier: 'platinum', 'gold', or 'bronze'\n",
    "        queries_per_day: Daily query volume\n",
    "        revenue_per_hour_downtime: Revenue lost per hour of downtime (in ₹)\n",
    "    \"\"\"\n",
    "    print(f\"Migration Cost Analysis\")\n",
    "    print(f\"Tenant tier: {tenant_tier.upper()}\")\n",
    "    print(f\"Query volume: {queries_per_day:,}/day\")\n",
    "    print(f\"Revenue impact: ₹{revenue_per_hour_downtime:,}/hour downtime\")\n",
    "    print()\n",
    "    \n",
    "    # Zero-downtime migration\n",
    "    zd_dev_cost = 50_00_000  # ₹50 lakh\n",
    "    zd_op_cost = 65_000  # ₹65K per migration\n",
    "    zd_downtime_hours = 0\n",
    "    zd_revenue_loss = zd_downtime_hours * revenue_per_hour_downtime\n",
    "    zd_total = zd_dev_cost + zd_op_cost + zd_revenue_loss\n",
    "    \n",
    "    print(\"ZERO-DOWNTIME MIGRATION:\")\n",
    "    print(f\"  Development: ₹{zd_dev_cost:,}\")\n",
    "    print(f\"  Operational: ₹{zd_op_cost:,}\")\n",
    "    print(f\"  Revenue loss: ₹{zd_revenue_loss:,} ({zd_downtime_hours} hours)\")\n",
    "    print(f\"  TOTAL: ₹{zd_total:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Maintenance window migration\n",
    "    mw_dev_cost = 3_50_000  # ₹3.5 lakh\n",
    "    mw_op_cost = 10_000  # ₹10K per migration\n",
    "    mw_downtime_hours = 3  # 3 hour maintenance window\n",
    "    mw_revenue_loss = mw_downtime_hours * revenue_per_hour_downtime\n",
    "    mw_total = mw_dev_cost + mw_op_cost + mw_revenue_loss\n",
    "    \n",
    "    print(\"MAINTENANCE WINDOW MIGRATION:\")\n",
    "    print(f\"  Development: ₹{mw_dev_cost:,}\")\n",
    "    print(f\"  Operational: ₹{mw_op_cost:,}\")\n",
    "    print(f\"  Revenue loss: ₹{mw_revenue_loss:,} ({mw_downtime_hours} hours)\")\n",
    "    print(f\"  TOTAL: ₹{mw_total:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"RECOMMENDATION:\")\n",
    "    if revenue_per_hour_downtime > 1_00_000:  # ₹1 lakh/hour\n",
    "        print(\"  → Use ZERO-DOWNTIME (high revenue impact)\")\n",
    "    elif tenant_tier == 'platinum':\n",
    "        print(\"  → Use ZERO-DOWNTIME (SLA requirement)\")\n",
    "    elif queries_per_day < 10_000:\n",
    "        print(\"  → Use MAINTENANCE WINDOW (low volume tenant)\")\n",
    "    else:\n",
    "        print(\"  → Use MAINTENANCE WINDOW (cost-effective)\")\n",
    "    print()\n",
    "    print(f\"Cost difference: ₹{abs(zd_total - mw_total):,}\")\n",
    "    print(f\"ROI breakeven: {(zd_dev_cost - mw_dev_cost) / max(1, mw_revenue_loss - zd_revenue_loss):.1f} migrations\")\n",
    "\n",
    "# Example scenarios\n",
    "print(\"Scenario 1: High-value Platinum tenant\")\n",
    "calculate_migration_costs(\n",
    "    tenant_tier='platinum',\n",
    "    queries_per_day=500_000,\n",
    "    revenue_per_hour_downtime=3_00_000  # ₹3 lakh/hour\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Scenario 2: Low-volume Bronze tenant\")\n",
    "calculate_migration_costs(\n",
    "    tenant_tier='bronze',\n",
    "    queries_per_day=5_000,\n",
    "    revenue_per_hour_downtime=10_000  # ₹10K/hour\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: GCC Enterprise Context\n",
    "\n",
    "**What is a Global Capability Center (GCC)?**\n",
    "Offshore/nearshore service delivery centers (e.g., Bangalore, Pune, Hyderabad) serving multiple business units of multinational corporations. Common in:\n",
    "- Financial services (JPMorgan, Goldman Sachs, HSBC)\n",
    "- Technology (Microsoft, Google, Amazon)\n",
    "- Manufacturing (Siemens, GE, Bosch)\n",
    "\n",
    "**Why Tenant Lifecycle Management is Critical at GCC Scale:**\n",
    "\n",
    "1. **Multi-BU Complexity:** Single GCC serves 50-200 business units, each a separate tenant\n",
    "2. **Cross-Region Operations:** Tenants span US, EU, APAC with different regulatory requirements\n",
    "3. **Cost Attribution:** Finance requires per-tenant cost tracking for chargeback\n",
    "4. **Compliance Layers:** Must satisfy parent company regulations + India operations + client contracts\n",
    "5. **SLA Diversity:** Platinum tenants pay 5-10x more than Bronze, expect different treatment\n",
    "\n",
    "**Stakeholder Perspectives:**\n",
    "\n",
    "**CFO (Finance):**\n",
    "- \"What's the ROI of ₹50 lakh zero-downtime investment?\"\n",
    "- \"Can we do tiered approach? Platinum gets zero-downtime, Bronze gets maintenance window?\"\n",
    "- Needs: Cost-benefit analysis, chargeback model, budget forecast\n",
    "\n",
    "**CTO (Technology):**\n",
    "- \"What's our migration success rate? Rollback frequency?\"\n",
    "- \"How do we prevent another 6-hour outage like last quarter?\"\n",
    "- Needs: Reliability metrics, incident post-mortems, capacity planning\n",
    "\n",
    "**Compliance (Legal/Risk):**\n",
    "- \"How do we prove GDPR deletion to EU regulators?\"\n",
    "- \"What if tenant is under legal hold during deletion request?\"\n",
    "- Needs: Audit trails, deletion certificates, legal hold workflow\n",
    "\n",
    "**Production Deployment Checklist (GCC Standards):**\n",
    "1. ✓ Technical review: Architecture, code quality, test coverage\n",
    "2. ✓ Security review: PII handling, encryption, access controls\n",
    "3. ✓ Compliance review: GDPR, SOC2, HIPAA applicability\n",
    "4. ✓ Business review: Cost justification, ROI analysis, SLA impact\n",
    "5. ✓ Governance review: Runbooks, on-call rotation, escalation paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Summary and Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Blue-Green Migration Pattern**\n",
    "- Six-phase orchestration: provision → sync → dual-write → catchup → cutover → decommission\n",
    "- Gradual traffic cutover (10% → 25% → 50% → 100%) for safe rollback\n",
    "- Sub-60 second rollback capability for failed migrations\n",
    "- Data consistency verification using multi-system checksums\n",
    "\n",
    "**2. GDPR Deletion Workflow**\n",
    "- Systematic erasure across 7+ systems (PostgreSQL, Redis, Pinecone, S3, CloudWatch, backups, analytics)\n",
    "- Legal hold checks preventing deletion during litigation\n",
    "- Multi-system verification to catch incomplete deletion\n",
    "- Cryptographically signed certificates (SHA-256) for audit trails\n",
    "\n",
    "**3. Backup/Restore/Clone Operations**\n",
    "- Tiered backup strategies by tenant SLA (Platinum: daily, Gold: weekly, Bronze: monthly)\n",
    "- Point-in-time recovery using transaction logs + base backups\n",
    "- Cross-region replication for disaster recovery\n",
    "- Tenant cloning with PII anonymization for staging/testing\n",
    "\n",
    "**4. Decision Framework**\n",
    "- When to use zero-downtime (₹40-60 lakh dev cost) vs maintenance window (₹2-5 lakh)\n",
    "- Cost-benefit analysis by tenant tier and revenue impact\n",
    "- Trade-offs: complexity, latency, cost vs availability and customer trust\n",
    "\n",
    "### Systems You Built\n",
    "\n",
    "1. **Migration Orchestrator:** Blue-green deployment with 6-phase workflow\n",
    "2. **GDPR Deletion Engine:** Multi-system deletion with verification and certificates\n",
    "3. **Backup/Restore Service:** Per-tenant snapshots with PITR and cross-region replication\n",
    "4. **Verification Framework:** Checksum validation and deletion completeness scans\n",
    "\n",
    "### Career Relevance\n",
    "\n",
    "**Roles where this matters:**\n",
    "- **Senior DevOps Engineer:** Design and implement zero-downtime deployment strategies\n",
    "- **Site Reliability Engineer (SRE):** Build backup/restore, incident response, disaster recovery\n",
    "- **Platform Engineer:** Multi-tenant lifecycle management at scale\n",
    "- **Compliance Engineer:** GDPR/HIPAA/SOC2 deletion workflows and audit trails\n",
    "\n",
    "**Interview Questions You Can Answer:**\n",
    "- \"How would you migrate a multi-TB tenant database with zero downtime?\"\n",
    "- \"Design a GDPR-compliant deletion system for a SaaS platform\"\n",
    "- \"What's your approach to backup/restore testing in production?\"\n",
    "- \"How do you handle rollback for failed migrations?\"\n",
    "\n",
    "### Next Module: M14.4 Operating Model & Governance\n",
    "\n",
    "**What's next:**\n",
    "- Multi-GCC operating models with cost allocation and chargeback\n",
    "- SLA management and tiered support (Platinum/Gold/Bronze)\n",
    "- Incident response and on-call rotation\n",
    "- Compliance frameworks (SOC2, HIPAA, ISO 27001)\n",
    "- Capacity planning and resource optimization\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "1. **Extend the migration orchestrator:**\n",
    "   - Add parallel sync workers (scale from 1 to 16)\n",
    "   - Implement sync progress monitoring\n",
    "   - Add automated rollback on error rate spike\n",
    "\n",
    "2. **Enhance GDPR deletion:**\n",
    "   - Add support for 8th system (e.g., Elasticsearch)\n",
    "   - Implement log anonymization (PII masking)\n",
    "   - Build deletion request approval workflow\n",
    "\n",
    "3. **Backup optimization:**\n",
    "   - Implement backup compression (reduce storage cost)\n",
    "   - Add backup encryption (compliance requirement)\n",
    "   - Build restore testing automation (monthly drills)\n",
    "\n",
    "4. **Cost optimization:**\n",
    "   - Build tiered migration strategy (Platinum/Gold/Bronze)\n",
    "   - Implement backup lifecycle policies (archive old backups to Glacier)\n",
    "   - Add cost tracking per tenant (chargeback model)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **GDPR Article 17:** [https://gdpr-info.eu/art-17-gdpr/](https://gdpr-info.eu/art-17-gdpr/)\n",
    "- **AWS Well-Architected Framework:** Reliability pillar (backup/restore)\n",
    "- **Google SRE Book:** Chapter 18 - Software Engineering in SRE\n",
    "- **PostgreSQL Replication:** Logical replication for zero-downtime migrations\n",
    "- **Terraform Blue-Green Modules:** Infrastructure as Code for dual environments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

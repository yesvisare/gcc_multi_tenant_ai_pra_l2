{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "learning-arc",
   "metadata": {},
   "source": [
    "# L3 M14.1: Multi-Tenant Monitoring & Observability\n",
    "\n",
    "## Learning Arc\n",
    "\n",
    "**Purpose:** Global dashboards lie by averaging. When 9 tenants operate at 50ms latency and 1 tenant runs at 5000ms, the platform average shows ~545ms‚Äîmasking the failing tenant completely. This module teaches you to implement tenant-aware monitoring that prevents \"averaging blindness\" and detects individual tenant failures.\n",
    "\n",
    "**Concepts Covered:**\n",
    "- Tenant-aware metrics with Prometheus (Counter, Histogram, Gauge, Info)\n",
    "- Label-based multi-tenancy pattern\n",
    "- Cardinality management (preventing metric explosion)\n",
    "- Drill-down Grafana dashboards\n",
    "- Distributed trace context propagation\n",
    "- SLA budget tracking per tenant\n",
    "- Resource monopolization detection\n",
    "- Per-tenant alerting and routing\n",
    "\n",
    "**After Completing This Notebook:**\n",
    "- You will understand how global averages hide individual tenant failures\n",
    "- You can implement per-tenant metrics using Prometheus labels\n",
    "- You will recognize cardinality explosion risks and mitigation strategies\n",
    "- You can build drill-down dashboards for tenant isolation\n",
    "- You will track SLA budgets and fire per-tenant alerts\n",
    "- You can detect resource monopolization (\"noisy neighbor\" problem)\n",
    "- You will propagate tenant context in distributed traces\n",
    "\n",
    "**Context in Track L3.M14:**\n",
    "This module builds on **L3 M13 (Multi-Tenant Architecture Patterns)** and prepares you for **L3 M14.2 (Incident Response & Runbooks)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path for imports\n",
    "if './src' not in sys.path:\n",
    "    sys.path.insert(0, './src')\n",
    "\n",
    "# OFFLINE mode for L3 consistency\n",
    "OFFLINE = os.getenv(\"OFFLINE\", \"false\").lower() == \"true\"\n",
    "\n",
    "# Prometheus detection\n",
    "PROMETHEUS_ENABLED = os.getenv(\"PROMETHEUS_ENABLED\", \"false\").lower() == \"true\"\n",
    "\n",
    "if OFFLINE or not PROMETHEUS_ENABLED:\n",
    "    print(\"‚ö†Ô∏è Running in OFFLINE/PROMETHEUS_DISABLED mode\")\n",
    "    print(\"   ‚Üí Metrics will be stored in-memory\")\n",
    "    print(\"   ‚Üí Set PROMETHEUS_ENABLED=true in .env to enable Prometheus server\")\n",
    "else:\n",
    "    print(\"‚úÖ Online mode - Prometheus metrics enabled\")\n",
    "\n",
    "# Import our monitoring module\n",
    "from l3_m14_monitoring_observability import (\n",
    "    start_query_tracking,\n",
    "    end_query_tracking,\n",
    "    track_query,\n",
    "    update_quota_usage,\n",
    "    get_tenant_metrics,\n",
    "    TenantMetricsCollector\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q65yxx9v4f9",
   "source": "## 1. Introduction & The Hook Problem (2-3 min)\n\n### The Averaging Blindness Problem\n\nImagine you're monitoring a multi-tenant RAG platform with 50 tenants. Your dashboard shows:\n- **Platform Average Latency:** 545ms\n- **Platform Success Rate:** 99%\n- **Platform CPU:** 60%\n\nEverything looks healthy! ‚úÖ\n\nBut here's the reality:\n- **49 tenants:** 50ms latency, 100% success rate\n- **1 tenant (Finance):** 5000ms latency, 80% success rate\n\n**The Problem:** Global averages completely hide the failing tenant.\n\n**The Impact:**\n- Finance team's SLA is being violated\n- You discover the issue 45 minutes later via an angry email\n- By then, the tenant has lost trust in the platform",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4m38c7h0aal",
   "source": "# Demonstration: How averaging hides outliers\n\n# Simulate 50 tenants\nhealthy_tenants = [50] * 49  # 49 tenants at 50ms\nfailing_tenant = [5000]      # 1 tenant at 5000ms\n\nall_latencies = healthy_tenants + failing_tenant\nplatform_average = sum(all_latencies) / len(all_latencies)\n\nprint(\"üîç Multi-Tenant Latency Analysis\")\nprint(\"=\" * 50)\nprint(f\"Healthy tenants (49): {healthy_tenants[0]}ms each\")\nprint(f\"Failing tenant (1):   {failing_tenant[0]}ms\")\nprint(f\"\\nüìä Platform Average: {platform_average:.1f}ms\")\nprint(\"\\n‚ùå Problem: The 5000ms outlier is completely hidden!\")\nprint(\"   ‚Üí Finance team experiences terrible performance\")\nprint(\"   ‚Üí Dashboard shows 'healthy' 545ms average\")\nprint(\"   ‚Üí SLA violation goes undetected for 45+ minutes\")\n\n# Expected: Shows ~545ms average hiding the 5000ms outlier",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f0kabbglsr9",
   "source": "## 2. Conceptual Foundation (4-5 min)\n\n### The Label-Based Multi-Tenancy Pattern\n\nInstead of one metric per tenant (which doesn't scale), Prometheus uses **labels** to add tenant context to metrics:\n\n```prometheus\n# Single metric definition serves ALL tenants\nrag_queries_total{tenant_id=\"finance\", status=\"success\"} 10250\nrag_queries_total{tenant_id=\"marketing\", status=\"success\"} 8900\n```\n\n**Key Concepts:**\n\n1. **Counter:** One-directional tracking (queries processed, errors occurred)\n2. **Histogram:** Distribution measurement (latency buckets, token counts)\n3. **Gauge:** Up/down values (active queries, quota usage percentage)\n4. **Info:** Static metadata (tenant name, tier, region)\n\n### Cardinality Management\n\n**The Rule:** Limit label cardinality to <1000 unique values per label\n\n‚úÖ **Safe:** `{tenant_id}` with 50 tenants ‚Üí 50 values  \n‚ùå **Unsafe:** `{tenant_id, user_id, query_id}` ‚Üí 50 √ó 10K √ó 100K = 50 billion time series\n\n**Result of explosion:** Prometheus runs out of memory, queries timeout, alerting fails",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7olxs66q1hi",
   "source": "# Demonstration: Cardinality calculation\n\n# Safe approach: tenant_id only\nsafe_tenants = 50\nsafe_statuses = 2  # success, error\nsafe_cardinality = safe_tenants * safe_statuses\n\n# Unsafe approach: adding high-cardinality labels\nunsafe_tenants = 50\nunsafe_users = 10000\nunsafe_queries = 100000\nunsafe_cardinality = unsafe_tenants * unsafe_users * unsafe_queries\n\nprint(\"üî¢ Cardinality Analysis\")\nprint(\"=\" * 60)\nprint(f\"\\n‚úÖ SAFE: {{tenant_id, status}}\")\nprint(f\"   {safe_tenants} tenants √ó {safe_statuses} statuses = {safe_cardinality:,} time series\")\nprint(f\"   ‚Üí Prometheus handles this easily\")\nprint(f\"\\n‚ùå UNSAFE: {{tenant_id, user_id, query_id}}\")\nprint(f\"   {unsafe_tenants} √ó {unsafe_users} √ó {unsafe_queries} = {unsafe_cardinality:,} time series\")\nprint(f\"   ‚Üí Prometheus OOM (Out of Memory)\")\nprint(f\"   ‚Üí Query timeouts\")\nprint(f\"   ‚Üí Alerting failures\")\nprint(f\"\\nüí° Solution: Use logging for high-cardinality data (user_id, query_id)\")\n\n# Expected: Shows 100 vs 50 billion series comparison",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1u7r4wzzsp1",
   "source": "## 3. Technical Implementation (12-15 min)\n\n### Pattern 1: Start/End Query Tracking\n\nThe most common pattern: Track query lifecycle with context propagation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tr3c7i03teo",
   "source": "# Pattern 1: Start/End tracking for finance team query\n\nprint(\"üìä Pattern 1: Start/End Query Tracking\")\nprint(\"=\" * 60)\n\n# Start tracking\ncontext = start_query_tracking(\"finance-team\")\nprint(f\"\\n‚úÖ Started tracking for: {context['tenant_id']}\")\nprint(f\"   Start time: {context['start_time']}\")\n\n# Simulate query execution\nprint(\"\\n‚è≥ Simulating RAG query execution...\")\ntime.sleep(0.5)  # Simulate 500ms query\n\n# End tracking with metrics\nend_query_tracking(\n    context,\n    status=\"success\",\n    docs_retrieved=5,\n    llm_tokens=1200\n)\n\nprint(\"\\n‚úÖ Query completed and metrics recorded:\")\nprint(\"   - Query counter incremented (finance-team, success)\")\nprint(\"   - Duration histogram updated (~0.5s)\")\nprint(\"   - Active queries gauge decremented\")\nprint(\"   - Docs retrieved: 5\")\nprint(\"   - LLM tokens: 1200\")\n\n# Expected: Shows tracking lifecycle with tenant isolation",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "58ljmliz0dm",
   "source": "### Pattern 2: Unified Track Query\n\nFor retroactive tracking or log-based metric ingestion",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "reafjm00qfl",
   "source": "# Pattern 2: Unified tracking for marketing team\n\nprint(\"\\nüìä Pattern 2: Unified Track Query\")\nprint(\"=\" * 60)\n\n# Track completed query in one call\ntrack_query(\n    tenant_id=\"marketing-team\",\n    status=\"success\",\n    duration=1.5,\n    docs_retrieved=3,\n    llm_tokens=800\n)\n\nprint(\"\\n‚úÖ Query tracked for marketing-team:\")\nprint(\"   Duration: 1.5s\")\nprint(\"   Docs: 3\")\nprint(\"   Tokens: 800\")\nprint(\"   Status: success\")\nprint(\"\\nüí° Use case: Backfilling metrics from event logs\")\n\n# Expected: Single-call tracking for simplified usage",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zxuqru4gbxq",
   "source": "### Pattern 3: Quota Usage Tracking\n\nMonitor resource consumption against tenant limits",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vu0fx530uh",
   "source": "# Pattern 3: Quota usage tracking\n\nprint(\"\\nüìä Pattern 3: Quota Usage Tracking\")\nprint(\"=\" * 60)\n\n# Finance team has used 7,500 of 10,000 monthly queries\nupdate_quota_usage(\"finance-team\", \"queries\", 75.0)\n\n# Marketing team has used 250,000 of 500,000 tokens\nupdate_quota_usage(\"marketing-team\", \"tokens\", 50.0)\n\nprint(\"\\n‚úÖ Quota metrics updated:\")\nprint(\"   finance-team: 75% query quota used\")\nprint(\"   marketing-team: 50% token quota used\")\nprint(\"\\nüí° Use case: Alert when usage > 90% before hitting hard limit\")\n\n# Expected: Quota gauge metrics recorded per tenant",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1r3yv7s649d",
   "source": "### Multi-Tenant Isolation Demonstration\n\nSimulate multiple tenants with different performance characteristics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ztefnlyka0c",
   "source": "# Simulate queries from 3 different tenants\n\nprint(\"\\nüè¢ Multi-Tenant Simulation\")\nprint(\"=\" * 60)\n\n# Finance team: High-performing tenant\nprint(\"\\nüíº Finance Team (Premium):\")\nfor i in range(3):\n    track_query(\"finance-team\", \"success\", 0.8, 5, 1200)\nprint(\"   ‚Üí 3 queries, ~0.8s each, 100% success\")\n\n# Marketing team: Standard tenant\nprint(\"\\nüì£ Marketing Team (Standard):\")\nfor i in range(2):\n    track_query(\"marketing-team\", \"success\", 1.5, 3, 800)\nprint(\"   ‚Üí 2 queries, ~1.5s each, 100% success\")\n\n# Engineering team: Experiencing issues\nprint(\"\\n‚öôÔ∏è Engineering Team (Free):\")\ntrack_query(\"engineering-team\", \"error\", 0.3, 0, 0)\ntrack_query(\"engineering-team\", \"success\", 2.5, 2, 500)\nprint(\"   ‚Üí 2 queries: 1 error + 1 slow success\")\n\nprint(\"\\nüìä Each tenant's metrics are ISOLATED\")\nprint(\"   ‚Üí Finance's performance doesn't affect Marketing\")\nprint(\"   ‚Üí Engineering's errors are visible per-tenant\")\nprint(\"   ‚Üí No averaging blindness!\")\n\n# Expected: Shows isolated tracking for each tenant",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dxrspwah9zf",
   "source": "### Retrieving Tenant Metrics\n\nQuery aggregated metrics for a specific tenant",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xry8tww50kh",
   "source": "# Retrieve metrics for each tenant\n\nprint(\"\\nüìà Retrieving Per-Tenant Metrics\")\nprint(\"=\" * 60)\n\ntenants = [\"finance-team\", \"marketing-team\", \"engineering-team\"]\n\nfor tenant in tenants:\n    metrics = get_tenant_metrics(tenant)\n    print(f\"\\n{tenant}:\")\n    print(f\"   Total queries: {metrics.get('total_queries', 0)}\")\n    \n    if 'success_count' in metrics:\n        print(f\"   Success: {metrics['success_count']}\")\n        print(f\"   Errors: {metrics['error_count']}\")\n        print(f\"   Avg duration: {metrics.get('avg_duration_seconds', 0):.3f}s\")\n\nprint(\"\\nüí° With Prometheus enabled, query directly:\")\nprint(\"   rate(rag_queries_total{tenant_id=\\\"finance-team\\\"}[5m])\")\n\n# Expected: Shows per-tenant aggregated metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y8v9ca2y3a",
   "source": "## 4. Common Failures & Debugging (4-5 min)\n\n### Failure Scenario 1: Averaging Blindness\n\n**Problem:** Platform shows 99% success; one tenant at 80% (others at 100%)\n\n**Detection:**\n```promql\n# Per-tenant success rate\nsum(rate(rag_queries_total{status=\"success\"}[5m])) by (tenant_id)\n/\nsum(rate(rag_queries_total[5m])) by (tenant_id)\n```\n\n**Alert:** `rag_success_rate{tenant_id} < 0.95`\n\n### Failure Scenario 2: Resource Monopolization\n\n**Problem:** One tenant consuming 40% CPU, slowing 49 others\n\n**Detection:**\n```promql\nsum(rate(rag_active_queries[1m])) by (tenant_id) > 100\n```\n\n**Fix:** Implement per-tenant rate limiting\n\n### Failure Scenario 3: Latency Masking\n\n**Problem:** Global 545ms hides 5000ms outlier\n\n**Detection:**\n```promql\nhistogram_quantile(0.99, rate(rag_query_duration_seconds_bucket[5m]))\n```\n\n**Alert:** `p99_latency{tenant_id} > 5s`\n\n### Failure Scenario 4: Cardinality Explosion\n\n**Problem:** Labeling by `{tenant_id, user_id, query_id}` creates billions of series\n\n**Solution:** Limit labels to `tenant_id` only; use logging for high-cardinality data\n\n### Failure Scenario 5: SLA Violation Detection Delay\n\n**Problem:** Without per-tenant error budgets, discovering violation takes 45+ minutes\n\n**Solution:** Per-tenant alerts fire within 3 minutes:\n```promql\nrag_quota_usage_percent{resource_type=\"queries\"} > 90\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8slbp144txs",
   "source": "## 5. Decision Card (3-4 min)\n\n### When to Use Multi-Tenant Monitoring\n\n**√¢≈ì‚Ä¶ Use this pattern when:**\n- You have 10+ tenants with different SLA targets\n- Individual tenant failures are hidden by platform averages\n- You need to attribute costs (tokens, queries) per tenant\n- Regulatory compliance requires tenant data isolation\n- You're experiencing \"noisy neighbor\" resource contention\n- You need drill-down capabilities from platform ‚Üí tenant ‚Üí user\n\n**‚ùå When NOT to use:**\n- Single-tenant system (use standard Prometheus metrics)\n- Fewer than 5 tenants (overhead exceeds benefit)\n- All tenants have identical SLAs and resource limits\n- You can't manage label cardinality (risk of metric explosion)\n- Your monitoring infrastructure can't handle 50-500 time series per tenant\n\n### Trade-offs\n\n| Aspect | Single-Tenant Monitoring | Multi-Tenant Monitoring |\n|--------|-------------------------|-------------------------|\n| **Cost** | Low (1 metric set) | Medium (50 tenants √ó 10 metrics = 500 series) |\n| **Latency** | No overhead | Minimal (label filtering in PromQL) |\n| **Complexity** | Simple (one dashboard) | Higher (per-tenant dashboards + cardinality mgmt) |\n| **Visibility** | Global only | Global + per-tenant drill-down |\n| **SLA Detection** | Slow (45+ minutes) | Fast (3 minutes per-tenant alerts) |\n| **Cardinality Risk** | None | Must manage to <1000 unique labels |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "1r8ffg8sdsn",
   "source": "## 6. Conclusion & Next Steps (2 min)\n\n### What You Learned\n\n√¢≈ì‚Ä¶ **The Problem:** Global averages hide individual tenant failures (\"averaging blindness\")\n\n√¢≈ì‚Ä¶ **The Solution:** Label-based multi-tenancy with Prometheus\n- Counter: Track query counts per tenant\n- Histogram: Measure latency distributions per tenant\n- Gauge: Monitor active queries and quota usage\n- Info: Store tenant metadata\n\n√¢≈ì‚Ä¶ **Key Patterns:**\n1. Start/End tracking with context propagation\n2. Unified tracking for retroactive metric ingestion\n3. Quota usage monitoring\n4. Per-tenant alerting\n\n√¢≈ì‚Ä¶ **Critical Rules:**\n- Limit label cardinality to <1000 unique values\n- Use `tenant_id` only; log high-cardinality data (user_id, query_id)\n- Alert per-tenant to detect SLA violations within 3 minutes\n\n### Production Checklist\n\nBefore deploying to production:\n\n- [ ] Prometheus server configured with 15-day retention\n- [ ] Grafana dashboards created with per-tenant drill-down\n- [ ] AlertManager rules configured for each tenant\n- [ ] Cardinality limits enforced (<1000 labels per metric)\n- [ ] Alert routing configured to tenant-specific channels\n- [ ] OpenTelemetry instrumentation adds `tenant_id` to traces\n- [ ] SLA budgets defined per tenant tier (free, standard, premium)\n\n### Next Modules\n\n- **L3 M14.2:** Incident Response & Runbooks\n- **L3 M14.3:** Cost Attribution & Chargeback\n- **L3 M14.4:** Capacity Planning & Forecasting\n\n### Additional Resources\n\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)\n- [Grafana Multi-Tenancy](https://grafana.com/docs/grafana/latest/administration/datasource-management/)\n- [Cardinality Management](https://www.robustperception.io/cardinality-is-key)\n\n---\n\n**üéì Congratulations!** You now understand how to implement tenant-aware monitoring and avoid the \"averaging blindness\" problem that plagues multi-tenant systems.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lfxguchhcym",
   "source": "# Final Summary: The Value of Per-Tenant Monitoring\n\nprint(\"\\nüéØ Key Takeaway: Per-Tenant Visibility\")\nprint(\"=\" * 60)\nprint(\"\\n‚ùå WITHOUT tenant-aware monitoring:\")\nprint(\"   - Platform shows: 545ms avg, 99% success\")\nprint(\"   - Reality: 1 tenant at 5000ms, 80% success\")\nprint(\"   - Detection time: 45+ minutes\")\nprint(\"   - Impact: Lost customer trust\")\nprint(\"\\n√¢≈ì‚Ä¶ WITH tenant-aware monitoring:\")\nprint(\"   - Alert: finance-team p99 > 5s\")\nprint(\"   - Alert: finance-team success rate < 95%\")\nprint(\"   - Detection time: 3 minutes\")\nprint(\"   - Impact: Proactive remediation before customer notices\")\nprint(\"\\nüí° The difference: Tenant isolation in metrics\")\nprint(\"   ‚Üí rag_queries_total{tenant_id=\\\"finance\\\"}\")\nprint(\"   ‚Üí rag_query_duration_seconds{tenant_id=\\\"finance\\\"}\")\nprint(\"\\n‚úÖ Notebook complete! Try the FastAPI server:\")\nprint(\"   ./scripts/run_api.ps1\")\nprint(\"   Then visit: http://localhost:8000/docs\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
# Module 14: Operations & Governance
## Video M14.2: Incident Management & Blast Radius (Enhanced with TVH Framework v2.0)

**Duration:** 35 minutes
**Track:** GCC Multi-Tenant Architecture for RAG Systems
**Level:** L2 SkillElevate (Builds on M11-M14.1)
**Audience:** Platform engineers managing multi-tenant RAG systems in GCC environments
**Prerequisites:** 
- GCC Multi-Tenant M11-M14.1 completed
- Understanding of circuit breaker patterns
- Basic incident response knowledge
- Familiarity with monitoring and alerting

---

## SECTION 1: INTRODUCTION & HOOK (2-3 minutes, 450 words)

**[0:00-0:30] Hook - The Production Nightmare**

[SLIDE: Title - "Incident Management & Blast Radius - When One Tenant Breaks Everything"]

**NARRATION:**
"It's 2:47 AM. Your phone explodes with alerts. 'Platform outage - ALL 50 tenants down.' You log in. Error rate: 98%. Users locked out. Revenue bleeding: â‚¹5 crore per hour across all tenants.

The investigation reveals the culprit: Tenant A - a media company - deployed a bad query that creates an infinite loop. Their single mistake cascaded across the entire platform, taking down 49 innocent tenants who did nothing wrong.

This is the nightmare scenario every GCC platform engineer fears: the blast radius problem. In a multi-tenant system, one failing tenant can become a bomb that explodes across the entire platform.

You've built multi-tenant isolation in M11-M13. You've implemented resource quotas, rate limiting, and namespace separation. But none of that helps when a tenant finds a way to consume infinite resources or trigger a cascade failure. 

The question isn't IF a tenant will fail - it's WHEN. And when they do, you have seconds - not minutes - to detect, isolate, and contain the blast radius before losing all 50 tenants.

Today, we're building an incident management system with automatic blast radius containment."

**INSTRUCTOR GUIDANCE:**
- Open with the visceral 2 AM scenario
- Emphasize the cascade effect (1 tenant â†’ 50 tenants)
- Show the cost impact (â‚¹5Cr/hour)
- Frame this as inevitable, not hypothetical

---

**[0:30-1:30] What We're Building Today**

[SLIDE: Blast Radius Containment Architecture showing:
- Failing tenant detection system (<60 seconds)
- Circuit breaker pattern (automatic isolation)
- Incident priority framework (P0/P1/P2)
- Automated tenant notification system
- Incident dashboard with real-time status]

**NARRATION:**
"Here's what we're building today:

An incident management system that detects failing tenants within 60 seconds, automatically isolates them using circuit breakers, and notifies affected tenants - all before the platform-wide outage spreads.

**Key capabilities:**

1. **Blast Radius Detection:** Monitors error rates across all 50 tenants, detects anomalies (>50% error rate) within 60 seconds

2. **Circuit Breaker Isolation:** Automatically opens circuit for failing tenant, routing requests away while other 49 tenants continue unaffected

3. **Incident Priority Calculator:** Determines P0/P1/P2 severity based on tenant tier and number affected

4. **Automated Notifications:** Sends alerts to ops team and affected tenant admins within 5 minutes of detection

5. **Runbook Templates:** Pre-built playbooks for 10+ common multi-tenant failure scenarios

By the end of this video, you'll have a complete incident management system that contains blast radius, isolates failing tenants automatically, and protects the other 49 tenants from cascade failures - turning a â‚¹5Cr platform outage into a â‚¹10L single-tenant incident."

**INSTRUCTOR GUIDANCE:**
- Show the architecture visually
- Emphasize the automatic nature (no manual intervention needed)
- Highlight the cost difference (â‚¹5Cr â†’ â‚¹10L)
- Make the 60-second detection SLA concrete

---

**[1:30-2:30] Learning Objectives**

[SLIDE: Learning Objectives with measurable outcomes]

**NARRATION:**
"In this video, you'll learn:

1. **Design blast radius detection:** Implement monitoring that identifies failing tenants within 60 seconds based on error rate thresholds (>50%)

2. **Implement circuit breaker pattern:** Build automatic isolation system with three states (closed/open/half-open) that trips after 5 consecutive failures

3. **Create incident priority framework:** Calculate P0/P1/P2 severity based on tenant tier (platinum/gold/silver) and number of affected tenants

4. **Build automated notification system:** Send alerts to ops team and tenant admins within 5 minutes of incident detection

5. **Design blameless postmortems:** Create templates that focus on system improvements, not individual blame

These aren't theoretical concepts. You're building production-grade incident response that prevents â‚¹5Cr platform outages by containing blast radius to single-tenant failures.

Let's start by understanding why blast radius matters in multi-tenant systems."

**INSTRUCTOR GUIDANCE:**
- Make objectives measurable (60 seconds, >50%, 5 failures)
- Connect to cost impact (â‚¹5Cr â†’ â‚¹10L)
- Preview the production-grade output
- Set expectation: this is about automatic detection and isolation

---

## SECTION 2: CONCEPTUAL FOUNDATION (8-10 minutes, 1,500 words)

**[2:30-4:00] Blast Radius Concept**

[SLIDE: Blast radius visualization showing:
- Single-tenant failure (isolated)
- Cascade failure (spreads to all tenants)
- Containment strategy (circuit breaker isolation)
- Impact comparison: 1 tenant down vs. 50 tenants down
- Cost comparison: â‚¹10L vs. â‚¹5Cr]

**NARRATION:**
"Let's talk about blast radius - the scope of damage from a single failure.

**Military Analogy:**
In military operations, 'blast radius' is the area affected by an explosion. A grenade has a 5-meter blast radius. A bomb has a 500-meter blast radius. Same concept in multi-tenant systems.

**Single-Tenant System (Small Blast Radius):**
When your system crashes, only YOUR users are affected. Blast radius = 1 company.

**Multi-Tenant System (Potentially Massive Blast Radius):**
When Tenant A crashes, it CAN take down Tenants B, C, D... all the way to Z. Blast radius = 50 companies.

**Why Multi-Tenant Failures Cascade:**

1. **Shared Infrastructure:** All tenants share compute, memory, network. Tenant A's infinite loop consumes all CPU â†’ other tenants starve

2. **Database Contention:** Tenant A's bad query locks tables â†’ other tenants' queries wait â†’ timeout â†’ cascade failure

3. **Queue Saturation:** Tenant A floods message queue â†’ other tenants' messages stuck â†’ system grinds to halt

4. **Circuit Breaker Tripping:** One tenant's failures trip shared circuit breakers â†’ all tenants blocked

**The Cost Math:**

Scenario 1 - No Blast Radius Containment:
- Tenant A fails (2:47 AM)
- Cascade spreads to all 50 tenants (2:50 AM)
- Platform down for 3 hours (until 5:47 AM)
- Impact: 50 tenants Ã— 3 hours = 150 tenant-hours
- Cost: â‚¹5Cr lost productivity + â‚¹50L emergency response + reputation damage = â‚¹5.5Cr+

Scenario 2 - With Blast Radius Containment:
- Tenant A fails (2:47 AM)
- Detection triggers (2:47:18 - 18 seconds)
- Circuit breaker opens (2:47:45 - 27 seconds later)
- Tenant A isolated (down 12 minutes total until fix)
- Other 49 tenants: unaffected, running normally
- Impact: 1 tenant Ã— 0.2 hours = 0.2 tenant-hours
- Cost: â‚¹10L (Tenant A only) + â‚¹5L emergency response = â‚¹15L

**Savings: â‚¹5.5Cr - â‚¹15L = â‚¹5.35Cr**

That's 36x cost reduction through blast radius containment.

**The Detection Challenge:**

You need to detect failures FAST:
- 60 seconds or less to identify failing tenant
- Before cascade spreads beyond isolation point
- With low false positive rate (<10%)

**The Isolation Challenge:**

You need to isolate AUTOMATICALLY:
- Circuit breaker opens without human intervention
- Other tenants continue unaffected
- Failed tenant gets notification within 5 minutes

This is the engineering challenge we're solving today."

**INSTRUCTOR GUIDANCE:**
- Use the military analogy to make blast radius concrete
- Show the cost math clearly (â‚¹5.5Cr vs â‚¹15L)
- Emphasize the speed requirement (60 seconds)
- Make it clear: automation is mandatory, not optional

---

**[4:00-6:00] Circuit Breaker Pattern**

[SLIDE: Circuit breaker state machine showing:
- Three states: Closed (normal), Open (isolated), Half-Open (testing recovery)
- State transitions with conditions
- Failure threshold (5 consecutive failures)
- Timeout period (60 seconds)
- Recovery verification]

**NARRATION:**
"The circuit breaker pattern is your automatic isolation mechanism. It works exactly like electrical circuit breakers in your home.

**Electrical Circuit Breaker:**
- Normal: Power flows
- Overload detected: Breaker trips instantly
- Power cut off: Prevents fire
- Manual reset: After issue resolved

**Software Circuit Breaker (Multi-Tenant Context):**

**State 1: CLOSED (Normal Operation)**
- Tenant requests flow normally
- System monitors error rate
- Failure counter tracks consecutive failures
- Threshold: 5 consecutive failures trips breaker

**State 2: OPEN (Isolated)**
- All requests to failing tenant BLOCKED
- Returns immediate error: 'Tenant isolated due to high failure rate'
- Other 49 tenants unaffected
- Timeout: 60 seconds before attempting recovery

**State 3: HALF-OPEN (Testing Recovery)**
- Allow limited requests through (10% traffic)
- If successful: transition back to CLOSED
- If still failing: back to OPEN for another 60 seconds

**Why This Matters in Multi-Tenant:**

Without circuit breaker:
- Tenant A's bad query hits database repeatedly
- Each query locks tables for 30 seconds
- Other tenants' queries pile up waiting
- Database connection pool exhausted
- Platform-wide outage

With circuit breaker:
- Tenant A's first 5 queries fail (5 seconds total)
- Circuit breaker trips on 5th failure
- Tenant A isolated (all subsequent requests blocked)
- Database recovers (no more bad queries)
- Other 49 tenants continue normally

**The Failure Threshold Trade-Off:**

Too low (2 failures): False positives frequent, unnecessary isolations
Too high (20 failures): Damage already done, cascade spreads
Sweet spot (5 failures): Balances speed vs. accuracy

**The Timeout Trade-Off:**

Too short (10 seconds): Premature recovery attempts, thrashing
Too long (300 seconds): Extended downtime for failed tenant
Sweet spot (60 seconds): Gives system time to recover, not excessive

**Multi-Tenant Specific Considerations:**

1. **Per-Tenant Circuit Breakers:** Each tenant has their own circuit breaker, isolated from others

2. **Shared vs. Dedicated Resources:** Circuit breakers on shared components (database, queue) must not punish all tenants

3. **Graceful Degradation:** When circuit opens, return meaningful error to tenant admin, not generic 500 error

4. **Audit Trail:** Every circuit breaker trip logged for postmortem analysis

This pattern is the foundation of blast radius containment."

**INSTRUCTOR GUIDANCE:**
- Use electrical circuit breaker as relatable analogy
- Walk through all three states clearly
- Explain the threshold and timeout trade-offs
- Emphasize per-tenant isolation (not platform-wide)

---

**[6:00-8:00] Incident Priority Framework**

[SLIDE: Incident priority decision tree showing:
- P0 (Critical): Platinum tenant affected OR 10+ tenants affected
- P1 (High): Gold tenant affected OR 5-9 tenants affected  
- P2 (Medium): Silver/Bronze tenant affected, <5 tenants
- Response SLAs: P0=15min, P1=60min, P2=4hr
- Escalation paths for each priority level]

**NARRATION:**
"Not all incidents are created equal. Your response time depends on incident priority, which is calculated based on two factors: tenant tier and blast radius.

**Tenant Tiers (from M13.1):**

- **Platinum:** Highest-paying customers, dedicated resources, 99.99% SLA
- **Gold:** High-value customers, prioritized resources, 99.9% SLA
- **Silver:** Standard customers, shared resources, 99% SLA
- **Bronze:** Basic tier, shared resources, best-effort SLA

**Priority Calculation Rules:**

**P0 - Critical (All Hands On Deck):**
- ANY platinum tenant affected, OR
- 10+ tenants affected (regardless of tier)
- Response SLA: 15 minutes
- Escalation: CTO notified immediately
- War room: Opened within 30 minutes

**P1 - High (Urgent Response):**
- Gold tenant affected, OR
- 5-9 tenants affected
- Response SLA: 60 minutes
- Escalation: Platform lead notified
- War room: Opened if not resolved in 90 minutes

**P2 - Medium (Normal Response):**
- Silver or Bronze tenant affected
- <5 tenants affected
- Response SLA: 4 hours
- Escalation: On-call engineer handles
- War room: Only if escalates to P1

**Why Tier Matters:**

Platinum tenant (â‚¹2Cr annual contract):
- Downtime cost: â‚¹50L/hour
- SLA penalties: â‚¹10L per hour beyond 99.99%
- Reputation risk: Fortune 500 client
- Priority: P0 always

Bronze tenant (â‚¹5L annual contract):
- Downtime cost: â‚¹50K/hour
- SLA: Best-effort (no penalties)
- Reputation risk: Limited
- Priority: P2 unless widespread

**Automated Priority Assignment:**

The system calculates priority automatically:

```python
def calculate_incident_priority(affected_tenants):
    # Step 1: Get highest tenant tier affected
    tier_priorities = {
        'platinum': 1,  # Highest urgency
        'gold': 2,
        'silver': 3,
        'bronze': 4
    }
    
    highest_tier = min(tier_priorities[t.tier] for t in affected_tenants)
    num_affected = len(affected_tenants)
    
    # Step 2: Apply priority rules
    if highest_tier == 1 or num_affected >= 10:
        return "P0"  # Critical
    elif highest_tier == 2 or num_affected >= 5:
        return "P1"  # High
    else:
        return "P2"  # Medium
```

**Real Scenario:**

Incident: Database connection pool exhausted
- 3 platinum tenants affected
- 12 gold tenants affected  
- 8 silver tenants affected
- Total: 23 tenants

Priority calculation:
- Highest tier: Platinum (tier=1) â†’ P0 candidate
- Number affected: 23 tenants â†’ P0 candidate
- Result: **P0 Critical**
- Response: CTO paged, war room opened, all hands on deck

**Why This Framework Matters:**

Without priority framework:
- All incidents treated equally
- Engineers overwhelmed
- Critical customers wait same time as basic tier
- SLA violations on platinum accounts

With priority framework:
- Clear response expectations
- Resource allocation matches customer value
- Platinum customers get 15-min response
- Bronze customers get reasonable 4-hour response
- Everyone knows their SLA"

**INSTRUCTOR GUIDANCE:**
- Explain tier-based prioritization rationally
- Show the cost/contract differences between tiers
- Make the automated calculation concrete
- Emphasize fairness (not all customers are equal, and that's OK in B2B)

---

**[8:00-10:00] Blameless Postmortems**

[SLIDE: Blameless postmortem template showing:
- Incident timeline (detection, response, resolution)
- Root cause analysis (5 whys technique)
- Contributing factors (system, not people)
- Action items with owners and deadlines
- Prevention strategies for future]

**NARRATION:**
"After every incident, you need a postmortem. But the wrong approach creates fear, hiding, and repeated failures. The right approach creates learning, transparency, and prevention.

**Blame Culture (What NOT To Do):**

'Who broke it?' mindset:
- Focus on individual who deployed
- Punishment or performance reviews
- Result: People hide issues, delay reporting, cover up

**Blameless Culture (The Right Way):**

'What broke and why?' mindset:
- Focus on system failures, not people
- Assumption: Everyone did their best with available information
- Result: Honest discussion, rapid learning, prevention

**Why Blameless Matters in Multi-Tenant:**

Multi-tenant systems are COMPLEX:
- 50+ tenants with different usage patterns
- Shared infrastructure with emergent behaviors
- Scale hides edge cases until production

Example incident:
- Tenant A deployed bad query
- Circuit breaker didn't trip (threshold too high)
- Database locked up (query timeout too long)
- Platform down 2 hours

**Blame approach:**
'Tenant A engineer made a mistake. Penalize them.'
Result: Tenant A stops deploying, stifles innovation

**Blameless approach:**
'Why did our system allow this query? Why didn't circuit breaker trip faster? What's the systemic fix?'
Result:
1. Lower circuit breaker threshold (20 â†’ 5 failures)
2. Add query timeout (60 sec â†’ 10 sec)
3. Implement query validator before deployment
4. Create tenant query testing environment

**The 5 Whys Technique:**

Incident: Platform outage from Tenant A query

Why #1: Why did platform go down?
â†’ Database locked up from bad query

Why #2: Why did bad query reach database?
â†’ No query validation before deployment

Why #3: Why no query validation?
â†’ We didn't have query validator tool

Why #4: Why no validator tool?
â†’ We prioritized features over safety tools

Why #5: Why did we deprioritize safety?
â†’ No incident history to justify investment

Root Cause: Lack of proactive safety investment before incident proves necessity

**Postmortem Template Structure:**

1. **Incident Summary** (2-3 sentences)
   - What broke, when, impact

2. **Timeline** (minute-by-minute)
   - 2:47 AM: First alerts
   - 2:50 AM: Engineer paged
   - 3:15 AM: Root cause identified
   - 5:47 AM: Fix deployed, platform recovered

3. **Root Cause** (5 whys)
   - System failure, not person failure

4. **Contributing Factors**
   - What made it worse?
   - Circuit breaker threshold too high
   - No query testing environment
   - Late detection (3 minutes, not 60 seconds)

5. **Action Items** (with owners and deadlines)
   - [Owner: Sarah] Lower circuit breaker threshold by Nov 20
   - [Owner: Team] Build query validator by Dec 1
   - [Owner: Platform] Improve detection speed to <60sec by Nov 25

6. **What Went Well**
   - Celebrate good responses (team coordination, communication)

7. **Lessons Learned**
   - System improvements, not individual blame

**The Key Principle:**

'Our job is to build systems so reliable that even when humans make mistakes, the system catches and contains them.'

That's blameless culture. Focus on system resilience, not individual perfection."

**INSTRUCTOR GUIDANCE:**
- Contrast blame vs. blameless clearly
- Show the 5 whys technique concretely
- Emphasize learning, not punishment
- Make the postmortem template actionable
- Connect to multi-tenant complexity

---

## SECTION 3: TECHNOLOGY STACK (2-3 minutes, 400 words)

**[10:00-12:00] Tools for Incident Management**

[SLIDE: Technology stack diagram with logos:
- Detection: Prometheus (metrics), Grafana (dashboards)
- Isolation: Python circuit breaker (pybreaker library)
- Notification: PagerDuty, Slack webhooks, email
- Logging: PostgreSQL (audit trail), ELK Stack
- Orchestration: Python (FastAPI background tasks)]

**NARRATION:**
"Let's talk about the technology stack for incident management. These are production-grade tools used in real GCC environments.

**Detection Layer:**

**Prometheus** (Metrics Collection)
- Why: Time-series metrics for all 50 tenants
- What it does: Tracks error rates, latency, request counts per tenant
- Key feature: Tenant labeling (automatic tagging with `tenant_id`)
- Query example: `rate(errors_total{tenant_id="A"}[5m]) > 0.5` (error rate >50% for Tenant A)

**Grafana** (Visualization & Alerting)
- Why: Real-time dashboards + alert rules
- What it does: Shows platform health, per-tenant metrics, trend analysis
- Key feature: Alert manager integration (triggers PagerDuty, Slack)

**Isolation Layer:**

**Pybreaker** (Circuit Breaker Library)
- Why: Production-tested circuit breaker pattern in Python
- What it does: Manages state (closed/open/half-open), tracks failures, automatic tripping
- Key feature: Per-tenant circuit breaker instances
- Alternative: We'll build custom solution for learning, but pybreaker is production choice

**Notification Layer:**

**PagerDuty** (Incident Management)
- Why: Industry standard for on-call management
- What it does: Pages engineers, tracks acknowledgment, escalation
- Key feature: Priority-based routing (P0 â†’ CTO, P1 â†’ Lead, P2 â†’ On-call)

**Slack Webhooks** (Team Communication)
- Why: Real-time team notifications
- What it does: Posts incident updates to #incidents channel
- Key feature: Thread-based discussion for each incident

**Email** (Tenant Notifications)
- Why: Formal notification to tenant admins
- What it does: Sends incident report with timeline, impact, ETA
- Key feature: Templates for different incident types

**Audit Trail Layer:**

**PostgreSQL** (Incident Database)
- Why: Relational database for incident records
- What it does: Stores incident timeline, actions taken, postmortem data
- Key feature: Links to circuit breaker trips, alert history

**ELK Stack** (Elasticsearch, Logstash, Kibana)
- Why: Centralized logging for investigation
- What it does: Aggregates logs from all services, searchable by tenant_id
- Key feature: Correlation of events across services

**Orchestration Layer:**

**Python FastAPI** (Background Tasks)
- Why: Async incident response automation
- What it does: Runs detection checks, triggers circuit breakers, sends notifications
- Key feature: Background task queues for non-blocking operations

**Cost Considerations:**

Open Source Stack (Self-Hosted):
- Prometheus + Grafana: Free
- Python + Pybreaker: Free
- PostgreSQL: Free
- ELK Stack: Free (compute costs only)
- Total: â‚¹50K/month (compute infrastructure)

Managed Stack (SaaS):
- Prometheus + Grafana Cloud: â‚¹30K/month
- PagerDuty: â‚¹25K/month (10 users)
- Slack: Included in enterprise plan
- PostgreSQL (AWS RDS): â‚¹40K/month
- Elasticsearch Cloud: â‚¹60K/month
- Total: â‚¹155K/month

Most GCCs choose: Hybrid (self-hosted monitoring, managed PagerDuty)
- Reason: Control over metrics + proven on-call management
- Cost: â‚¹75K/month

These are the tools we're using today."

**INSTRUCTOR GUIDANCE:**
- Explain why each tool is chosen (not just what it does)
- Show open source vs. managed costs
- Note that we'll build custom for learning, but production uses libraries
- Connect tools to specific capabilities (detection, isolation, notification)

---

## SECTION 4: TECHNICAL IMPLEMENTATION (15-20 minutes, 3,500 words)

**[12:00-14:00] Blast Radius Detector Implementation**

[SLIDE: Detector architecture showing:
- Prometheus metrics query
- Error rate calculation per tenant
- Threshold comparison (>50%)
- Failing tenant identification
- Alert trigger mechanism]

**NARRATION:**
"Let's build the blast radius detector - the system that monitors all 50 tenants and identifies failures within 60 seconds.

**Design Approach:**

Every RAG query generates metrics:
- `queries_total{tenant_id="A"}` - total query count
- `queries_errors{tenant_id="A"}` - error count
- `queries_duration{tenant_id="A"}` - latency distribution

Error rate = errors / total queries over last 5 minutes

If error rate > 50% for any tenant â†’ ALERT

**Implementation Strategy:**

1. Background task runs every 10 seconds
2. Queries Prometheus for all tenant metrics
3. Calculates error rate per tenant
4. Identifies failing tenants (rate > 50%)
5. Triggers circuit breaker for failing tenant
6. Logs incident to database
7. Sends notifications

Let's build it."

```python
# blast_radius_detector.py
import time
import logging
from datetime import datetime, timedelta
from typing import List, Optional
import requests
from dataclasses import dataclass

# Configure logging for incident tracking
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TenantMetrics:
    """Store tenant health metrics"""
    tenant_id: str
    total_queries: int
    error_queries: int
    error_rate: float
    timestamp: datetime

class BlastRadiusDetector:
    """
    Monitors all tenants and detects failing tenants within 60 seconds.
    
    This is the first line of defense against cascade failures. By detecting
    high error rates quickly (>50% over 5 minutes), we can isolate failing
    tenants before they impact the other 49 tenants.
    
    Key Design Decisions:
    - 10-second polling interval: Balances detection speed (total 60 sec) vs. system load
    - 5-minute window: Reduces false positives from transient blips
    - 50% threshold: Clear signal of systemic failure, not random errors
    """
    
    def __init__(
        self,
        prometheus_url: str = "http://prometheus:9090",
        error_threshold: float = 0.50,  # 50% error rate triggers isolation
        check_window: str = "5m",  # Look at last 5 minutes of metrics
        check_interval: int = 10  # Poll every 10 seconds
    ):
        self.prometheus_url = prometheus_url
        self.error_threshold = error_threshold
        self.check_window = check_window
        self.check_interval = check_interval
        
        logger.info(
            f"BlastRadiusDetector initialized: "
            f"threshold={error_threshold*100}%, "
            f"window={check_window}, "
            f"interval={check_interval}s"
        )
    
    def get_all_tenants(self) -> List[str]:
        """
        Query Prometheus for all active tenant IDs.
        
        Uses Prometheus label values query to get unique tenant_id labels
        from the last hour of metrics. This ensures we monitor even tenants
        who haven't made recent requests.
        
        Returns:
            List of tenant IDs (e.g., ['A', 'B', 'C', ...])
        """
        try:
            # Query for all unique tenant_id label values
            # This captures every tenant who's made any request in the last hour
            query = f'{self.prometheus_url}/api/v1/label/tenant_id/values'
            response = requests.get(query, timeout=5)
            response.raise_for_status()
            
            data = response.json()
            if data['status'] == 'success':
                tenants = data['data']
                logger.debug(f"Found {len(tenants)} active tenants")
                return tenants
            else:
                logger.error(f"Failed to get tenant list: {data}")
                return []
                
        except Exception as e:
            logger.error(f"Error querying tenant list: {e}")
            return []
    
    def get_tenant_metrics(self, tenant_id: str) -> Optional[TenantMetrics]:
        """
        Calculate error rate for a specific tenant over the check window.
        
        Uses Prometheus rate() function to get per-second rate, then averages
        over the window. This smooths out transient spikes and gives a reliable
        signal of systemic failure.
        
        Args:
            tenant_id: The tenant to check (e.g., 'A')
            
        Returns:
            TenantMetrics object with error rate, or None if query fails
        """
        try:
            # Query 1: Total queries per second (averaged over window)
            # rate() gives per-second rate, so multiply by window seconds
            total_query = (
                f'sum(rate(rag_queries_total{{tenant_id="{tenant_id}"}}[{self.check_window}]))'
            )
            
            # Query 2: Error queries per second (averaged over window)
            error_query = (
                f'sum(rate(rag_queries_errors{{tenant_id="{tenant_id}"}}[{self.check_window}]))'
            )
            
            # Execute both queries
            total_result = self._execute_prometheus_query(total_query)
            error_result = self._execute_prometheus_query(error_query)
            
            if total_result is None or error_result is None:
                return None
            
            # Extract values from Prometheus response
            # Response format: [timestamp, "value_as_string"]
            total_queries = float(total_result[1])
            error_queries = float(error_result[1])
            
            # Avoid division by zero
            # If no queries in window, error rate is 0 (tenant is idle, not failing)
            if total_queries == 0:
                error_rate = 0.0
            else:
                error_rate = error_queries / total_queries
            
            return TenantMetrics(
                tenant_id=tenant_id,
                total_queries=int(total_queries * 300),  # Convert rate to count (5min window)
                error_queries=int(error_queries * 300),
                error_rate=error_rate,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Error getting metrics for tenant {tenant_id}: {e}")
            return None
    
    def _execute_prometheus_query(self, query: str) -> Optional[list]:
        """
        Execute a PromQL query and return the result value.
        
        Handles connection errors, timeouts, and malformed responses.
        Returns None on any error to allow graceful degradation.
        """
        try:
            url = f'{self.prometheus_url}/api/v1/query'
            response = requests.get(
                url,
                params={'query': query},
                timeout=5  # 5-second timeout prevents hanging
            )
            response.raise_for_status()
            
            data = response.json()
            if data['status'] == 'success' and data['data']['result']:
                # Return the first (and only) result value
                return data['data']['result'][0]['value']
            return None
            
        except Exception as e:
            logger.debug(f"Prometheus query failed: {e}")
            return None
    
    def detect_failing_tenant(self) -> Optional[str]:
        """
        Main detection loop: Check all tenants and identify failures.
        
        Iterates through all active tenants, calculates error rates,
        and returns the first tenant exceeding the error threshold.
        
        Critical: This returns FIRST failing tenant only. In a real incident,
        multiple tenants might be failing simultaneously. See Section 8 for
        handling multiple concurrent failures.
        
        Returns:
            Tenant ID of failing tenant, or None if all healthy
        """
        tenants = self.get_all_tenants()
        
        if not tenants:
            logger.warning("No active tenants found - check Prometheus connectivity")
            return None
        
        logger.debug(f"Checking {len(tenants)} tenants for failures")
        
        for tenant_id in tenants:
            metrics = self.get_tenant_metrics(tenant_id)
            
            if metrics is None:
                # Skip this tenant if metrics unavailable
                # Don't want false positives from monitoring failure
                continue
            
            logger.debug(
                f"Tenant {tenant_id}: {metrics.error_rate*100:.1f}% errors "
                f"({metrics.error_queries}/{metrics.total_queries} over {self.check_window})"
            )
            
            # CRITICAL DECISION: Is this tenant failing?
            if metrics.error_rate > self.error_threshold:
                logger.critical(
                    f"FAILING TENANT DETECTED: {tenant_id} "
                    f"({metrics.error_rate*100:.1f}% error rate > "
                    f"{self.error_threshold*100}% threshold)"
                )
                
                # Log full metrics for postmortem
                logger.info(f"Failure details: {metrics}")
                
                return tenant_id
        
        # All tenants healthy
        return None
    
    def run_detection_loop(self, circuit_breaker_manager):
        """
        Continuous monitoring loop that runs in background.
        
        This is the production deployment pattern:
        1. Check all tenants every 10 seconds
        2. If failing tenant found, trigger circuit breaker
        3. Log incident for postmortem
        4. Continue monitoring (don't exit on first failure)
        
        Args:
            circuit_breaker_manager: The CircuitBreakerManager instance
                                    that handles tenant isolation
        """
        logger.info("Starting blast radius detection loop")
        
        iteration = 0
        while True:
            try:
                iteration += 1
                start_time = time.time()
                
                # Main detection logic
                failing_tenant = self.detect_failing_tenant()
                
                if failing_tenant:
                    # CRITICAL: Tenant is failing, trigger isolation
                    logger.critical(
                        f"Iteration {iteration}: Failing tenant {failing_tenant} "
                        f"detected, triggering circuit breaker"
                    )
                    
                    # Pass to circuit breaker manager for isolation
                    circuit_breaker_manager.trip_breaker(failing_tenant)
                else:
                    # All tenants healthy
                    logger.debug(f"Iteration {iteration}: All tenants healthy")
                
                # Calculate how long this check took
                elapsed = time.time() - start_time
                logger.debug(f"Detection check completed in {elapsed:.2f}s")
                
                # Sleep until next check interval
                # Subtract elapsed time to maintain consistent polling
                sleep_time = max(0, self.check_interval - elapsed)
                time.sleep(sleep_time)
                
            except KeyboardInterrupt:
                logger.info("Detection loop stopped by user")
                break
            except Exception as e:
                # Log error but don't crash the detector
                # Monitoring system must be resilient
                logger.error(f"Error in detection loop: {e}", exc_info=True)
                time.sleep(self.check_interval)  # Continue after error


# Example usage in production
if __name__ == "__main__":
    # Initialize detector with production settings
    detector = BlastRadiusDetector(
        prometheus_url="http://prometheus:9090",
        error_threshold=0.50,  # 50% error rate
        check_window="5m",  # 5-minute window
        check_interval=10  # Check every 10 seconds
    )
    
    # In production, this would be passed from main app
    # For testing, we'll create a mock
    class MockCircuitBreakerManager:
        def trip_breaker(self, tenant_id):
            print(f"[MOCK] Circuit breaker tripped for {tenant_id}")
    
    breaker_manager = MockCircuitBreakerManager()
    
    # Start continuous monitoring
    # This runs forever in production (managed by systemd/supervisor)
    detector.run_detection_loop(breaker_manager)
```

**NARRATION:**
"Let's walk through what we just built:

**Detection Speed:**
- Check interval: 10 seconds
- Maximum detection time: 60 seconds (6 iterations worst case)
- Typical detection time: 18-30 seconds (depends on when failure starts relative to check)

**Why 5-Minute Window:**
If we used 1-minute window: Too sensitive, transient spikes cause false positives
If we used 15-minute window: Too slow, damage already done
5 minutes: Balances signal vs. noise

**Why 50% Threshold:**
Below 50%: Might be normal errors (network blips, user mistakes)
Above 50%: Clear systemic failure (bad query, resource exhaustion, bug)

**What This Detector Catches:**

Scenario 1: Bad Query (Infinite Loop)
- Tenant A deploys query with infinite recursion
- First few queries timeout (30 sec each)
- Within 2 minutes, error rate hits 80%
- Detection: ~30 seconds after first failure
- Action: Circuit breaker trips, Tenant A isolated

Scenario 2: Resource Exhaustion
- Tenant B's traffic spikes 10x unexpectedly
- Rate limiter blocks 90% of requests
- Error rate: 90%
- Detection: Within 20 seconds
- Action: Circuit breaker trips, Tenant B isolated

Scenario 3: Database Contention
- Tenant C's query locks critical table
- Other Tenant C queries wait â†’ timeout â†’ fail
- Error rate: 100%
- Detection: Within 15 seconds
- Action: Circuit breaker trips, Tenant C isolated

This detector is your early warning system."

**INSTRUCTOR GUIDANCE:**
- Walk through code comments explaining educational rationale
- Emphasize the speed (10-second checks, 60-second max detection)
- Show the threshold trade-offs (50% is the sweet spot)
- Connect to real failure scenarios

---

**[14:00-18:00] Circuit Breaker Implementation**

[SLIDE: Circuit breaker state machine flowchart with:
- State transitions (closedâ†’openâ†’half-openâ†’closed)
- Failure counting logic
- Timeout mechanism
- Recovery testing]

**NARRATION:**
"Now let's build the circuit breaker - the automatic isolation mechanism that protects the other 49 tenants.

**Design Approach:**

Each tenant gets their own circuit breaker instance:
- Independent state (Tenant A's failures don't affect Tenant B's circuit)
- Per-tenant failure counters
- Per-tenant timeout periods

**State Machine:**

State 1: CLOSED (Normal)
- Requests pass through
- Failures tracked
- After 5 consecutive failures â†’ transition to OPEN

State 2: OPEN (Isolated)
- All requests blocked immediately
- Return error: 'Tenant isolated due to high failure rate'
- After 60 seconds â†’ transition to HALF_OPEN

State 3: HALF_OPEN (Testing)
- Allow 10% of requests through
- If successful: reset counter, transition to CLOSED
- If still failing: back to OPEN for another 60 seconds

Let's implement it."

```python
# circuit_breaker.py
import time
import logging
from enum import Enum
from datetime import datetime, timedelta
from typing import Callable, Any, Optional
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)

class CircuitState(Enum):
    """Three states of the circuit breaker"""
    CLOSED = "closed"  # Normal operation, requests pass through
    OPEN = "open"  # Isolated, all requests blocked
    HALF_OPEN = "half_open"  # Testing recovery, limited requests pass

@dataclass
class CircuitBreakerStats:
    """Track circuit breaker metrics for monitoring and postmortems"""
    tenant_id: str
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    consecutive_failures: int = 0
    circuit_trips: int = 0  # How many times circuit has opened
    last_trip_time: Optional[datetime] = None
    last_recovery_time: Optional[datetime] = None
    time_in_open_state: float = 0.0  # Total seconds isolated


class CircuitBreakerOpen(Exception):
    """
    Custom exception raised when circuit breaker is OPEN.
    
    This allows calling code to handle isolation gracefully:
    - Return 503 Service Unavailable to tenant
    - Log incident for tenant admin
    - Show estimated recovery time
    """
    pass


class TenantCircuitBreaker:
    """
    Per-tenant circuit breaker for automatic isolation.
    
    This is the blast radius containment mechanism. When a tenant starts
    failing (>5 consecutive failures), we automatically isolate them to
    prevent cascade failure affecting other 49 tenants.
    
    Key Design Principles:
    1. Fast failure: Trip after 5 failures (not 20), minimize damage
    2. Automatic recovery: Don't require manual intervention
    3. Gradual testing: Half-open state tests with 10% traffic before full recovery
    4. Transparency: Log every state transition for postmortem analysis
    
    Production Considerations:
    - Each tenant has own circuit breaker (50 instances for 50 tenants)
    - State persists in memory (lost on restart, acceptable for this use case)
    - For persistent state across restarts, use Redis/PostgreSQL
    """
    
    def __init__(
        self,
        tenant_id: str,
        failure_threshold: int = 5,  # Trip after 5 consecutive failures
        timeout_duration: float = 60.0,  # Stay open for 60 seconds
        half_open_success_threshold: int = 3  # Need 3 successes to fully recover
    ):
        self.tenant_id = tenant_id
        self.failure_threshold = failure_threshold
        self.timeout_duration = timeout_duration
        self.half_open_success_threshold = half_open_success_threshold
        
        # Current state
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0  # Used in HALF_OPEN state
        
        # Timing
        self.opened_at: Optional[datetime] = None
        self.last_failure_at: Optional[datetime] = None
        
        # Statistics
        self.stats = CircuitBreakerStats(tenant_id=tenant_id)
        
        logger.info(
            f"CircuitBreaker created for tenant {tenant_id}: "
            f"threshold={failure_threshold}, timeout={timeout_duration}s"
        )
    
    def call(self, func: Callable, *args, **kwargs) -> Any:
        """
        Execute a function through the circuit breaker.
        
        This is the main entry point for all tenant requests:
        1. Check circuit state
        2. If OPEN: Raise exception (block request)
        3. If CLOSED/HALF_OPEN: Try to execute function
        4. Track success/failure
        5. Update circuit state based on result
        
        Args:
            func: The function to execute (e.g., RAG query)
            *args, **kwargs: Arguments to pass to func
            
        Returns:
            Result of func execution
            
        Raises:
            CircuitBreakerOpen: If circuit is OPEN (tenant isolated)
            Any exception from func: If function fails
        """
        self.stats.total_requests += 1
        
        # Check current state and whether we should attempt request
        if not self._can_attempt_request():
            # Circuit is OPEN, block request
            self.stats.failed_requests += 1
            raise CircuitBreakerOpen(
                f"Tenant {self.tenant_id} is isolated due to high failure rate. "
                f"Circuit opened at {self.opened_at}. "
                f"Estimated recovery: {self.opened_at + timedelta(seconds=self.timeout_duration)}"
            )
        
        # Attempt the request
        try:
            result = func(*args, **kwargs)
            
            # Success! Handle based on current state
            self._on_success()
            
            return result
            
        except Exception as e:
            # Failure! Handle based on current state
            self._on_failure()
            
            # Re-raise the original exception so caller can handle
            raise
    
    def _can_attempt_request(self) -> bool:
        """
        Determine if we should attempt this request based on circuit state.
        
        CLOSED: Always attempt
        HALF_OPEN: Attempt (testing recovery)
        OPEN: Check if timeout expired
            - If expired: Transition to HALF_OPEN, attempt
            - If not expired: Reject request
            
        Returns:
            True if request should be attempted, False if should be blocked
        """
        if self.state == CircuitState.CLOSED:
            return True
        
        if self.state == CircuitState.HALF_OPEN:
            return True
        
        if self.state == CircuitState.OPEN:
            # Check if timeout period has elapsed
            if self.opened_at and (datetime.now() - self.opened_at).total_seconds() >= self.timeout_duration:
                # Timeout expired, transition to HALF_OPEN for testing
                logger.info(
                    f"Circuit breaker timeout expired for tenant {self.tenant_id}, "
                    f"transitioning OPEN â†’ HALF_OPEN for recovery testing"
                )
                self._transition_to_half_open()
                return True
            else:
                # Still in timeout period, block request
                return False
        
        return False
    
    def _on_success(self):
        """
        Handle successful request based on current state.
        
        CLOSED:
            - Reset failure counter (streak broken)
        
        HALF_OPEN:
            - Increment success counter
            - If reached threshold (3 successes): Transition to CLOSED (fully recovered)
        
        OPEN:
            - Should never happen (requests blocked in OPEN state)
        """
        self.stats.successful_requests += 1
        
        if self.state == CircuitState.CLOSED:
            # Reset consecutive failure counter
            # One success breaks the failure streak
            if self.failure_count > 0:
                logger.debug(
                    f"Tenant {self.tenant_id}: Success after {self.failure_count} failures, "
                    f"resetting failure counter"
                )
                self.failure_count = 0
        
        elif self.state == CircuitState.HALF_OPEN:
            # Count successes during recovery testing
            self.success_count += 1
            logger.info(
                f"Tenant {self.tenant_id}: Success in HALF_OPEN state "
                f"({self.success_count}/{self.half_open_success_threshold})"
            )
            
            # Check if we've had enough successes to fully recover
            if self.success_count >= self.half_open_success_threshold:
                logger.info(
                    f"Tenant {self.tenant_id}: Recovery verified, "
                    f"transitioning HALF_OPEN â†’ CLOSED"
                )
                self._transition_to_closed()
    
    def _on_failure(self):
        """
        Handle failed request based on current state.
        
        CLOSED:
            - Increment failure counter
            - If reached threshold (5 failures): Trip circuit (CLOSED â†’ OPEN)
        
        HALF_OPEN:
            - Recovery failed, transition back to OPEN
            - Reset timeout period (another 60 seconds)
        
        OPEN:
            - Should never happen (requests blocked in OPEN state)
        """
        self.stats.failed_requests += 1
        self.stats.consecutive_failures += 1
        self.last_failure_at = datetime.now()
        
        if self.state == CircuitState.CLOSED:
            self.failure_count += 1
            logger.warning(
                f"Tenant {self.tenant_id}: Failure {self.failure_count}/{self.failure_threshold}"
            )
            
            # Check if we've hit the threshold
            if self.failure_count >= self.failure_threshold:
                logger.critical(
                    f"Tenant {self.tenant_id}: Failure threshold reached "
                    f"({self.failure_count} consecutive failures), "
                    f"TRIPPING CIRCUIT"
                )
                self._transition_to_open()
        
        elif self.state == CircuitState.HALF_OPEN:
            # Recovery failed, go back to OPEN
            logger.error(
                f"Tenant {self.tenant_id}: Failure during recovery testing, "
                f"transitioning HALF_OPEN â†’ OPEN"
            )
            self._transition_to_open()
    
    def _transition_to_open(self):
        """
        Transition circuit breaker to OPEN state (isolated).
        
        This is when blast radius containment activates:
        - All subsequent requests blocked
        - Other tenants unaffected
        - Incident logged for postmortem
        - Notifications sent to ops team and tenant admin
        """
        self.state = CircuitState.OPEN
        self.opened_at = datetime.now()
        self.stats.circuit_trips += 1
        self.stats.last_trip_time = self.opened_at
        
        logger.critical(
            f"ðŸš¨ CIRCUIT BREAKER TRIPPED: Tenant {self.tenant_id} ISOLATED"
        )
        logger.critical(
            f"   Reason: {self.failure_count} consecutive failures"
        )
        logger.critical(
            f"   Impact: Tenant {self.tenant_id} isolated, other tenants unaffected"
        )
        logger.critical(
            f"   Recovery: Automatic retry in {self.timeout_duration} seconds"
        )
        
        # TODO: In production, this would trigger:
        # - PagerDuty incident
        # - Slack notification to #incidents
        # - Email to tenant admin
        # - Update incident dashboard
    
    def _transition_to_half_open(self):
        """
        Transition to HALF_OPEN state for recovery testing.
        
        After timeout expires, we cautiously test if tenant has recovered.
        Don't immediately flood them with traffic - start with a few requests.
        """
        self.state = CircuitState.HALF_OPEN
        self.success_count = 0  # Reset success counter
        
        # Calculate how long we were isolated
        if self.opened_at:
            isolation_duration = (datetime.now() - self.opened_at).total_seconds()
            self.stats.time_in_open_state += isolation_duration
            
            logger.info(
                f"Tenant {self.tenant_id}: Testing recovery after {isolation_duration:.1f}s isolation"
            )
    
    def _transition_to_closed(self):
        """
        Transition back to CLOSED state (fully recovered).
        
        Tenant has proven they're healthy again:
        - 3 consecutive successes in HALF_OPEN
        - Reset all counters
        - Resume normal operation
        """
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.stats.last_recovery_time = datetime.now()
        
        logger.info(
            f"Ã¢Å“â€¦ CIRCUIT BREAKER RECOVERED: Tenant {self.tenant_id} back to normal"
        )
        logger.info(
            f"   Total trips: {self.stats.circuit_trips}"
        )
        logger.info(
            f"   Total isolation time: {self.stats.time_in_open_state:.1f}s"
        )
        
        # TODO: In production, this would trigger:
        # - Update incident dashboard (resolved)
        # - Notify tenant admin (service restored)
        # - Log for postmortem analysis
    
    def get_state_info(self) -> dict:
        """
        Get current circuit breaker state and statistics.
        
        Used by monitoring dashboards and incident management systems.
        
        Returns:
            Dictionary with state, stats, and timing information
        """
        info = {
            'tenant_id': self.tenant_id,
            'state': self.state.value,
            'failure_count': self.failure_count,
            'failure_threshold': self.failure_threshold,
            'stats': {
                'total_requests': self.stats.total_requests,
                'successful_requests': self.stats.successful_requests,
                'failed_requests': self.stats.failed_requests,
                'success_rate': (
                    self.stats.successful_requests / self.stats.total_requests * 100
                    if self.stats.total_requests > 0 else 0
                ),
                'circuit_trips': self.stats.circuit_trips,
                'total_isolation_time': self.stats.time_in_open_state
            }
        }
        
        if self.opened_at:
            info['opened_at'] = self.opened_at.isoformat()
            info['estimated_recovery'] = (
                self.opened_at + timedelta(seconds=self.timeout_duration)
            ).isoformat()
        
        return info
    
    def manual_trip(self, reason: str):
        """
        Manually trip the circuit breaker (emergency use).
        
        Used by ops team during incidents where automatic detection
        hasn't triggered yet but manual investigation shows tenant
        is causing problems.
        
        Example: Ops team sees tenant consuming 90% of database connections
        even though error rate is below threshold.
        """
        logger.warning(
            f"Manual circuit breaker trip for tenant {self.tenant_id}: {reason}"
        )
        self._transition_to_open()
    
    def manual_reset(self):
        """
        Manually reset circuit breaker (emergency use).
        
        Used after manual intervention (e.g., fixed tenant's bug, deployed patch).
        Bypasses normal recovery testing.
        
        Dangerous: Only use after confirming root cause is resolved.
        """
        logger.warning(
            f"Manual circuit breaker reset for tenant {self.tenant_id}"
        )
        self._transition_to_closed()


class CircuitBreakerManager:
    """
    Manages circuit breakers for all 50 tenants.
    
    Provides centralized access to per-tenant circuit breakers:
    - Lazy initialization (create breaker on first request)
    - Unified interface for detection system
    - Aggregated statistics across all tenants
    """
    
    def __init__(self):
        self.breakers: dict[str, TenantCircuitBreaker] = {}
        logger.info("CircuitBreakerManager initialized")
    
    def get_breaker(self, tenant_id: str) -> TenantCircuitBreaker:
        """
        Get circuit breaker for tenant (lazy initialization).
        
        Args:
            tenant_id: The tenant ID
            
        Returns:
            TenantCircuitBreaker instance for this tenant
        """
        if tenant_id not in self.breakers:
            self.breakers[tenant_id] = TenantCircuitBreaker(tenant_id)
            logger.debug(f"Created new circuit breaker for tenant {tenant_id}")
        
        return self.breakers[tenant_id]
    
    def trip_breaker(self, tenant_id: str):
        """
        Manually trip circuit breaker for tenant.
        
        Called by BlastRadiusDetector when high error rate detected.
        
        Args:
            tenant_id: The tenant to isolate
        """
        breaker = self.get_breaker(tenant_id)
        breaker.manual_trip(
            reason=f"High error rate detected by BlastRadiusDetector"
        )
    
    def get_all_breakers_status(self) -> list[dict]:
        """
        Get status of all circuit breakers.
        
        Used by incident dashboard to show health of all 50 tenants.
        
        Returns:
            List of dictionaries with per-tenant status
        """
        return [
            breaker.get_state_info()
            for breaker in self.breakers.values()
        ]
    
    def get_open_circuits(self) -> list[str]:
        """
        Get list of currently isolated tenants.
        
        Returns:
            List of tenant IDs with OPEN circuit breakers
        """
        return [
            tenant_id
            for tenant_id, breaker in self.breakers.items()
            if breaker.state == CircuitState.OPEN
        ]


# Example usage
if __name__ == "__main__":
    # Create manager
    manager = CircuitBreakerManager()
    
    # Simulate tenant request through circuit breaker
    def simulated_rag_query(query: str):
        """Simulated RAG query that might fail"""
        import random
        if random.random() < 0.6:  # 60% chance of failure
            raise Exception("Query timeout")
        return f"Result for: {query}"
    
    # Get breaker for Tenant A
    breaker = manager.get_breaker("A")
    
    # Simulate requests
    for i in range(10):
        try:
            result = breaker.call(simulated_rag_query, f"Query {i}")
            print(f"âœ“ Request {i}: Success - {result}")
        except CircuitBreakerOpen as e:
            print(f"âœ— Request {i}: Circuit OPEN - {e}")
        except Exception as e:
            print(f"âœ— Request {i}: Failed - {e}")
        
        time.sleep(0.5)
    
    # Show final stats
    print("\nFinal State:")
    print(breaker.get_state_info())
```

**NARRATION:**
"Let's understand what we built:

**Isolation Speed:**
- After 5th consecutive failure: Circuit trips immediately (<1 second)
- All subsequent requests blocked
- Other 49 tenants unaffected

**Recovery Process:**
- Wait 60 seconds (gives system time to recover)
- Transition to HALF_OPEN (testing mode)
- Try 3 requests
- If all 3 succeed: Fully recovered (CLOSED)
- If any fail: Back to OPEN for another 60 seconds

**Why This Works:**

Real Incident - Tenant A Bad Query:
- Request 1: Timeout (30 sec)
- Request 2: Timeout (30 sec)
- Request 3: Timeout (30 sec)
- Request 4: Timeout (30 sec)
- Request 5: Timeout (30 sec) â†’ Circuit trips

Total time before isolation: ~150 seconds (2.5 minutes)
Total requests attempted: 5
Database impact: Minimal (5 bad queries, then stopped)

Without circuit breaker:
- Bad query runs for 30 minutes (ops finally notices)
- 60 queries Ã— 30 sec each = 1,800 seconds of database lock
- Platform down for all 50 tenants

With circuit breaker:
- Bad query stopped after 150 seconds
- 5 queries Ã— 30 sec each = 150 seconds of database lock
- Tenant A isolated, other 49 tenants running

**Cost Impact:**

Without breaker: â‚¹5Cr platform outage (all 50 tenants)
With breaker: â‚¹10L single-tenant outage (only Tenant A)
Savings: â‚¹4.9Cr

This is why circuit breakers are mandatory in multi-tenant systems."

**INSTRUCTOR GUIDANCE:**
- Walk through state machine visually
- Emphasize the automatic nature (no human intervention)
- Show the real incident timeline (5 failures â†’ trip)
- Connect to cost savings (â‚¹5Cr â†’ â‚¹10L)
- Note that we built this for learning, but production uses libraries like pybreaker

---

**END OF PART 1**

This completes Sections 1-4. Parts 2 and 3 will follow with remaining sections.
# Module 14: Operations & Governance
## Video M14.2: Incident Management & Blast Radius - PART 2

**[Continuation from Part 1 - Sections 5-8]**

---

## SECTION 5: REALITY CHECK (3-4 minutes, 600 words)

**[18:00-21:00] Honest Limitations of Circuit Breakers**

[SLIDE: "Reality Check" with balance scale showing:
- What circuit breakers solve: Cascade failures, platform-wide outages
- What they don't solve: Root causes, tenant bugs, capacity issues]

**NARRATION:**
"Let's be brutally honest about what we just built. Circuit breakers are powerful, but they're not magic. They contain blast radius - they don't fix the underlying problems.

**Limitation #1: False Positives Are Inevitable**

**Scenario:** Major cloud provider has 2-minute regional outage
- ALL 50 tenants' requests fail (upstream issue, not tenant issue)
- Detection system sees 100% error rate across all tenants
- Circuit breakers trip for all 50 tenants
- Result: All tenants isolated even though none are at fault

**Metrics:**
- False positive rate: 5-10% of circuit breaker trips in production
- Manual reset required: ~30 minutes average (ops investigation + reset)
- Cost of false positive: â‚¹25L downtime (all tenants isolated unnecessarily)

**Mitigation:**
- Correlation detection: If >10 tenants failing simultaneously, likely platform issue, not tenant issue
- External health checks: Ping known-good endpoints, if they fail, don't blame tenants
- Manual override: Ops can bypass circuit breakers during platform incidents

**Trade-off:**
Accept 5-10% false positives to prevent 90-95% true positives (cascade failures). The alternative - no circuit breakers - is worse (â‚¹5Cr platform outages).

---

**Limitation #2: Circuit Breakers Don't Fix Root Causes**

**Scenario:** Tenant A has infinite loop query
- Circuit breaker trips, Tenant A isolated âœ“
- 60 seconds later, circuit tests recovery
- Bad query still deployed, fails again
- Circuit breaker trips again
- This repeats forever until Tenant A fixes their code

**Reality:**
- Circuit breakers buy time (prevent cascade)
- They don't fix bugs (tenant must fix their code)
- Mean Time to Recovery (MTTR): Depends on tenant responsiveness

**Measured Impact:**
- Quick tenants: Fix in 15 minutes (1-2 circuit breaker trips)
- Average tenants: Fix in 2 hours (5-10 circuit breaker trips)
- Slow tenants: Fix in 24 hours (50+ circuit breaker trips)

**Cost:**
- Tenant A: Down until fix deployed (their responsibility)
- Other 49 tenants: Unaffected (circuit breaker success)
- Ops team: Monitoring, escalation, communication (â‚¹5L labor cost)

**What We Actually Built:**
- Damage containment: Yes âœ“
- Automatic recovery: Only if tenant fixes issue
- Problem resolution: No (requires human intervention)

---

**Limitation #3: Noisy Neighbor Detection Isn't Foolproof**

**Scenario:** Tenant B is consuming 80% of database connections (but not failing)
- Their queries succeed (so circuit breaker doesn't trip)
- Other tenants starved for connections (slow queries, not errors yet)
- Eventually other tenants start failing (circuit breakers trip incorrectly)

**The Challenge:**
- Error rate monitoring: Only catches failures, not resource hogging
- Resource monitoring needed: CPU, memory, connections, disk I/O
- Multi-dimensional detection: Complex, requires advanced monitoring

**Current Solution Coverage:**
- Catches: Queries that fail (timeouts, errors)
- Misses: Queries that succeed but consume excessive resources

**Enhancement Needed (Out of Scope Today):**
- Per-tenant resource quotas (M12.3)
- Noisy neighbor detection (M12.3)
- Proactive throttling (before failures start)

**Trade-off:**
Today's solution: 80% of multi-tenant incidents caught
Full solution: 95%+ of incidents caught (requires additional systems from M12.3)

---

**Limitation #4: Recovery Testing Can Cause Thrashing**

**Scenario:** Tenant C's failure is intermittent (works 30% of time)
- Circuit trips after 5 failures
- 60 seconds later, test recovery
- 1 success, then 2 failures â†’ back to OPEN
- 60 seconds later, test again
- 1 success, 1 failure â†’ back to OPEN
- This repeats indefinitely (thrashing)

**Impact:**
- Tenant C: Intermittent service (frustrating UX)
- Ops team: Alert fatigue (50+ notifications)
- Dashboard: Looks chaotic (constant state changes)

**Measured Frequency:**
- Stable failures: 70% (clear pattern, clean recovery)
- Intermittent failures: 20% (thrashing behavior)
- Platform issues: 10% (false positives)

**Mitigation Strategies:**

1. **Exponential Backoff:** After 3 failed recovery attempts, increase timeout (60s â†’ 120s â†’ 240s)
2. **Success Threshold:** Require 5 consecutive successes (not 3) for intermittent cases
3. **Manual Intervention:** Flag thrashing cases for ops review (automated ticket creation)

**Why We Don't Implement These Today:**
- Adds complexity (state machine becomes complicated)
- Most incidents are stable (70% don't need it)
- Manual intervention works for edge cases (20%)

**Production Recommendation:**
Start with simple circuit breaker (what we built today), add sophistication after observing real incident patterns.

---

**Limitation #5: Blast Radius Containment Isn't Free**

**Costs:**

**Infrastructure:**
- Prometheus: â‚¹15K/month (metrics storage)
- Alerting system: â‚¹10K/month (PagerDuty)
- Dashboard: â‚¹5K/month (Grafana Cloud)
- Total: â‚¹30K/month

**Operational:**
- On-call rotation: 3 engineers Ã— â‚¹8L/year = â‚¹24L/year
- Incident response: ~10 hours/month Ã— â‚¹5K/hour = â‚¹50K/month = â‚¹6L/year
- Postmortems: ~5 hours/month Ã— â‚¹5K/hour = â‚¹25K/month = â‚¹3L/year
- Total: â‚¹33L/year

**Total Annual Cost: â‚¹30K Ã— 12 + â‚¹33L = â‚¹36.6L/year (~â‚¹3L/month)**

**ROI Calculation:**

Without circuit breakers:
- Platform outages: 4 per year (industry average)
- Cost per outage: â‚¹5Cr (all 50 tenants down 3 hours)
- Annual cost: 4 Ã— â‚¹5Cr = â‚¹20Cr

With circuit breakers:
- Platform outages: 0 per year (contained to single tenant)
- Single-tenant incidents: 40 per year (10x more frequent, but localized)
- Cost per incident: â‚¹10L (one tenant down 12 minutes)
- Annual cost: 40 Ã— â‚¹10L = â‚¹4Cr
- Circuit breaker cost: â‚¹3.6L
- Total: â‚¹4.36Cr

**Savings: â‚¹20Cr - â‚¹4.36Cr = â‚¹15.64Cr per year**

**ROI: 428x return on investment**

Worth it? Absolutely.

---

**The Honest Summary:**

Circuit breakers are 80% solution:
- Ã¢Å“" Prevent cascade failures
- Ã¢Å“" Automatic isolation
- Ã¢Å“" Cost-effective (428x ROI)
- Ã¢Å“â€” Don't fix root causes
- Ã¢Å“â€” 5-10% false positives
- Ã¢Å“â€” Require manual intervention for edge cases

But 80% solution beats 0% solution (no protection) by infinite margin.

In multi-tenant systems, circuit breakers are mandatory, not optional. The question isn't WHETHER to implement them - it's HOW sophisticated to make them.

Start simple (what we built today). Add complexity only after observing real failure patterns in your GCC."

**INSTRUCTOR GUIDANCE:**
- Be brutally honest about limitations
- Show real metrics (false positive rates, costs)
- Emphasize the ROI (428x return)
- Don't oversell circuit breakers as perfect solution
- Connect to other modules for full solution (M12.3)

---

## SECTION 6: ALTERNATIVE APPROACHES (3-4 minutes, 600 words)

**[21:00-24:00] Different Blast Radius Strategies**

[SLIDE: Comparison matrix showing:
- Approach 1: Circuit Breakers (our choice)
- Approach 2: Resource Quotas (preventive)
- Approach 3: Tenant Tiers (priority-based)
- Comparison: Speed, Cost, Complexity, Coverage]

**NARRATION:**
"Circuit breakers aren't the only way to contain blast radius. Let's compare three approaches and understand when each makes sense.

---

**Approach 1: Circuit Breakers (Reactive Isolation) - What We Built**

**How It Works:**
- Monitor error rates continuously
- Detect failing tenant (error rate >50%)
- Isolate automatically (trip circuit)
- Recover automatically (test after timeout)

**Pros:**
- Fast detection (<60 seconds)
- Automatic isolation (no human intervention)
- Works for unknown failure modes (any high error rate)
- Relatively simple implementation

**Cons:**
- Reactive (damage happens before detection)
- False positives (5-10%)
- Doesn't prevent resource exhaustion (only error-rate failures)
- Requires monitoring infrastructure

**Cost:**
- Monthly: â‚¹30K (monitoring)
- Annual labor: â‚¹33L (on-call, incidents)
- Total: â‚¹3.6L/month

**When to Use:**
- Mid-stage GCC (10-50 tenants)
- Unknown failure patterns (haven't observed enough incidents yet)
- Fast time-to-market (implement in 2 weeks)
- Budget-conscious (reasonable cost)

**Real Example:**
Media GCC with 30 tenants, unknown failure patterns, Ã¢â€šÂ¹40L annual budget
â†’ Circuit breakers: Best choice (covers 80% of incidents, affordable)

---

**Approach 2: Resource Quotas (Preventive Limits) - Covered in M12.3**

**How It Works:**
- Set hard limits per tenant: CPU, memory, connections, disk I/O
- Enforce at infrastructure level (Kubernetes resource limits)
- Tenant hits limit â†’ Throttled (not isolated)
- Other tenants unaffected (physical isolation)

**Pros:**
- Preventive (stops problems before they cascade)
- No false positives (limits are absolute)
- Catches resource exhaustion (not just errors)
- Predictable performance (guaranteed isolation)

**Cons:**
- Requires capacity planning (what limits per tenant?)
- Over-provision = wasted money, under-provision = tenant complaints
- Complex implementation (multi-dimensional limits)
- Doesn't catch all failure modes (e.g., bad queries that succeed but are slow)

**Cost:**
- Monthly: â‚¹80K (Kubernetes orchestration, monitoring)
- Annual labor: â‚¹40L (capacity planning, tuning)
- Over-provisioning waste: â‚¹20L/year (unused capacity)
- Total: â‚¹7L/month

**When to Use:**
- Large GCC (50+ tenants)
- Known resource patterns (historical data available)
- High-value tenants (platinum tier needs guarantees)
- Budget available (2x cost of circuit breakers)

**Real Example:**
FinTech GCC with 80 tenants, strict SLAs, Ã¢â€šÂ¹2Cr annual budget
â†’ Resource quotas: Best choice (guaranteed isolation, worth the cost)

---

**Approach 3: Tenant Tiers with Dedicated Resources - Enterprise Scale**

**How It Works:**
- Platinum tenants: Dedicated infrastructure (separate cluster)
- Gold tenants: Dedicated namespaces (shared cluster)
- Silver/Bronze: Shared infrastructure (best effort)

**Isolation:**
- Platinum: Physical isolation (can't affect each other)
- Gold: Namespace isolation (resource quotas per namespace)
- Silver/Bronze: No isolation (circuit breakers as safety net)

**Pros:**
- Complete isolation for high-value tenants
- Scalable (add more tiers as needed)
- Marketing advantage ('dedicated infrastructure for enterprise clients')
- Simple blast radius (platinum can't be affected by silver/bronze)

**Cons:**
- Very expensive (3x infrastructure cost)
- Complex operations (multiple clusters to manage)
- Under-utilization (dedicated resources often idle)
- Only worth it for huge GCCs (100+ tenants)

**Cost:**
- Monthly: â‚¹5L (dedicated infrastructure for 10 platinum tenants)
- Annual labor: â‚¹80L (managing multiple clusters)
- Total: â‚¹13L/month

**When to Use:**
- Huge GCC (100+ tenants)
- Mix of Fortune 500 clients (need dedicated) + SMBs (can share)
- Budget available (3-4x cost of circuit breakers)
- Marketing differentiator (sell 'dedicated infrastructure')

**Real Example:**
Banking GCC with 150 tenants (10 platinum, 40 gold, 100 silver/bronze), Ã¢â€šÂ¹10Cr budget
â†’ Tier-based: Best choice (platinum demands it, silver/bronze accepts shared)

---

**Decision Framework:**

**Step 1: Count Your Tenants**
- <10 tenants: Circuit breakers (overkill but good practice)
- 10-50 tenants: Circuit breakers (sweet spot)
- 50-100 tenants: Resource quotas + circuit breakers (hybrid)
- 100+ tenants: Tier-based (dedicated for high-value)

**Step 2: Analyze Your Budget**
- <â‚¹50L/year: Circuit breakers only (affordable)
- â‚¹50L-â‚¹2Cr/year: Resource quotas + circuit breakers (recommended)
- >â‚¹2Cr/year: Tier-based with dedicated infrastructure (enterprise)

**Step 3: Assess Your Risk**
- Low risk (internal GCC tools): Circuit breakers (good enough)
- Medium risk (B2B SaaS): Resource quotas (guaranteed isolation)
- High risk (regulated industries): Tier-based (maximum isolation)

**Step 4: Consider Your Timeline**
- Need now (2 weeks): Circuit breakers (quick to implement)
- Need soon (2 months): Resource quotas (requires planning)
- Future roadmap (6+ months): Tier-based (strategic investment)

**Hybrid Approach (Recommended for Most GCCs):**

**Phase 1 (Month 1-3):** Circuit breakers
- Cover 80% of incidents
- Low cost, fast implementation
- Learn failure patterns

**Phase 2 (Month 4-6):** Add resource quotas for top 10 tenants
- Based on observed patterns from Phase 1
- Prevents noisy neighbor issues
- Still affordable

**Phase 3 (Month 7-12):** Tier-based for platinum tenants
- Dedicated infrastructure for highest-value clients
- Marketing advantage
- Gradual cost increase

**Total Cost Trajectory:**
- Month 1-3: â‚¹3L/month (circuit breakers)
- Month 4-6: â‚¹5L/month (+ quotas for top 10)
- Month 7-12: â‚¹8L/month (+ dedicated for platinum)

Most GCCs follow this path. Start simple, add sophistication as budget and need grow.

---

**Our Choice for This Video: Circuit Breakers**

**Why:**
- Covers 80% of incidents
- Affordable (â‚¹3L/month)
- Quick to implement (2 weeks)
- Foundation for future enhancements

**What We're NOT Building Today (But You Might Need):**
- Resource quotas: See M12.3
- Tier-based architecture: See M13.1
- Advanced noisy neighbor detection: See M12.3

Circuit breakers are the starting point, not the ending point."

**INSTRUCTOR GUIDANCE:**
- Present all three approaches fairly
- Show clear cost comparisons
- Provide decision framework (tenant count, budget, risk)
- Recommend hybrid approach (phased implementation)
- Don't just advocate for circuit breakers (show alternatives)

---

## SECTION 7: WHEN NOT TO USE CIRCUIT BREAKERS (2 minutes, 400 words)

**[24:00-26:00] Scenarios Where Circuit Breakers Are Wrong Choice**

[SLIDE: Red flags - When circuit breakers cause more harm than good:
Ã¢Å’ Single-tenant systems (no blast radius to contain)
Ã¢Å’ Read-only systems (failures don't cascade)
Ã¢Å’ Already have physical isolation (dedicated infrastructure per tenant)
Ã¢Å’ False positives unacceptable (medical, financial, safety-critical)]

**NARRATION:**
"Let's be clear: circuit breakers aren't always the right choice. Here are scenarios where they cause more harm than good.

---

**Scenario #1: Single-Tenant Systems**

**Context:** You have dedicated RAG instance per customer (not multi-tenant)

**Why circuit breakers are overkill:**
- No blast radius (Tenant A can't affect Tenant B - separate infrastructure)
- Failure impact: Limited to one tenant (already isolated)
- Cost of circuit breaker: â‚¹30K/month
- Benefit: Zero (nothing to contain)

**What you actually need:**
- Basic monitoring (alert when tenant's instance fails)
- Auto-restart (systemd, Kubernetes liveness probes)
- Backups (restore failed instance)

**Better approach:**
- Kubernetes health checks: Free (built into K8s)
- Automatic pod restart: Free (K8s handles it)
- Savings: â‚¹30K/month (circuit breaker infrastructure)

---

**Scenario #2: Read-Only Systems (No State Mutations)**

**Context:** RAG system for document search only (no writes, no state changes)

**Why circuit breakers are less critical:**
- Read-only queries: Don't cause cascade failures (no lock contention)
- Failed read: Affects one user (no downstream impact)
- Retry safe: Can retry reads without side effects

**Example:** Public document search (like Arxiv papers, Wikipedia)
- User query fails: Show error message, user retries
- No cascade: Failed read doesn't block other reads
- No urgency: User can wait, not mission-critical

**Better approach:**
- Client-side retry: Browser/app retries on failure (3 attempts)
- CDN failover: Serve cached results if backend down
- Cost: â‚¹5K/month (CDN)

**When circuit breakers still make sense:**
- Very high query volume (10K+ QPS)
- Failure could exhaust resources (even for reads)
- But priority is lower than write-heavy systems

---

**Scenario #3: Already Have Physical Isolation**

**Context:** Platinum tenants on dedicated infrastructure (separate K8s cluster per tenant)

**Why circuit breakers are redundant:**
- Physical isolation: Tenant A on Cluster A, Tenant B on Cluster B
- No shared resources: Tenant A's failure can't affect Tenant B (separate hardware)
- Blast radius: Already contained by architecture

**Cost:**
- Dedicated infrastructure: â‚¹50L/month (10 platinum tenants)
- Circuit breakers: â‚¹30K/month (adds no value)

**Better investment:**
- Cluster monitoring (one cluster failing doesn't affect others)
- Auto-scaling (handle spikes without failure)
- DR replication (backup cluster for failover)

**Rule:** If architecture provides physical isolation, circuit breakers are insurance you don't need.

---

**Scenario #4: False Positives Unacceptable**

**Context:** Medical diagnosis RAG, financial trading RAG, safety-critical systems

**Problem with circuit breakers:**
- 5-10% false positive rate (trip when shouldn't)
- False positive impact: Legitimate tenant isolated incorrectly
- Cost of false positive: Missed medical diagnosis, failed trade, safety incident

**Example:** Hospital RAG system
- Scenario: Cloud provider hiccup (2 min regional outage)
- Circuit breakers trip: All hospital tenants isolated
- Impact: Doctors can't access patient records during emergency
- Outcome: Patient harm, malpractice lawsuit

**Risk:**
Circuit breaker 'false positive' cost > Platform outage cost

**Better approach:**
- Human-in-the-loop: Alert ops team, require manual confirmation before isolation
- Higher threshold: 95% error rate (not 50%) to reduce false positives
- Backup systems: Redundant infrastructure, failover clusters

**Trade-off:**
- Lower false positives (0.1% vs 5%)
- Slower detection (5 minutes vs 60 seconds)
- But: Safer for critical systems

---

**Scenario #5: Excessive Engineering (Pre-Mature Optimization)**

**Context:** 3-tenant startup, pre-product-market-fit

**Why circuit breakers are overkill:**
- Only 3 tenants (can manually monitor)
- Startup mode: Features matter more than reliability
- Cost: â‚¹30K/month circuit breakers + â‚¹50K/month engineer time
- Burn rate: Accelerated for minimal benefit

**Better approach:**
- Manual monitoring (founder/CTO watches dashboard)
- Simple alerts (Slack notification on errors)
- Fast iteration (ship features, worry about scale later)

**Rule:** Don't build multi-tenant incident management until you have 10+ tenants paying customers.

---

**The Decision:**

Circuit breakers make sense when:
âœ“ Multi-tenant architecture (shared infrastructure)
âœ“ Writes or state mutations (cascade risk)
âœ“ 10+ tenants (blast radius is real)
âœ“ False positives acceptable (5-10% OK)
âœ“ Budget available (â‚¹3L/month)

Circuit breakers don't make sense when:
Ã¢Å’ Single-tenant (already isolated)
Ã¢Å’ Read-only (no cascade risk)
Ã¢Å’ Physical isolation (architecture handles it)
Ã¢Å’ Safety-critical (false positives too dangerous)
Ã¢Å’ <10 tenants (manual monitoring sufficient)

Be honest about your context. Don't cargo-cult circuit breakers just because 'best practice'."

**INSTRUCTOR GUIDANCE:**
- Be blunt about when NOT to use circuit breakers
- Show cost of false positives in critical systems
- Emphasize context matters (not one-size-fits-all)
- Warn against premature optimization (startups)

---

## SECTION 8: COMMON FAILURES (2-3 minutes, 600 words)

**[26:00-29:00] What Goes Wrong in Production**

[SLIDE: Common failure patterns in multi-tenant incident management with real metrics]

**NARRATION:**
"Let's look at what actually breaks in production when managing incidents in multi-tenant systems. These are real failures from real GCCs.

---

**Failure #1: Detection Too Slow (Miss the Cascade Window)**

**What Happens:**
- Blast radius detector checks every 60 seconds (not 10 seconds)
- Tenant A starts failing at 2:47:00
- Next check at 2:48:00 (60 seconds later)
- By then, cascade already spreading (database locked, queue saturated)
- Circuit breaker trips at 2:48:05, but damage done

**Symptom:**
- Circuit breaker trips correctly
- But platform already partially degraded
- Other tenants showing elevated error rates (5-10%)

**Why It Happens:**
- Engineer optimizes for cost (reduce Prometheus queries)
- 60-second interval seems 'good enough'
- Until first real incident

**Root Cause:**
Check interval too long (60s) vs. cascade speed (30-45s)

**Fix:**
- Reduce check interval to 10 seconds (6x faster detection)
- Cost increase: â‚¹5K/month (more Prometheus queries)
- Benefit: Catch cascade before it spreads

**Prevention:**
- Measure cascade speed in load testing (how fast does failure spread?)
- Set check interval to 50% of cascade speed (safety margin)
- For 60-second cascade: 30-second checks minimum

**Incident Metrics:**
- Slow detection (60s): 23% of tenants affected before containment
- Fast detection (10s): 2% of tenants affected
- ROI: 10x fewer tenants impacted

---

**Failure #2: Circuit Breaker Threshold Too High**

**What Happens:**
- Threshold set to 20 consecutive failures (not 5)
- Tenant B's bad query runs 20 times before circuit trips
- Each query locks database table for 30 seconds
- Total damage: 20 Ã— 30 = 600 seconds (10 minutes) of lock
- By the time circuit trips, other tenants already timing out

**Symptom:**
- Circuit breaker eventually trips correctly
- But 'eventually' is too late (10 minutes of damage)

**Why It Happens:**
- Engineer worried about false positives
- Sets threshold high to be 'safe'
- Doesn't measure actual damage during threshold window

**Root Cause:**
Optimized for false positive rate (too conservative) vs. damage containment (too slow)

**Fix:**
- Lower threshold to 5 consecutive failures
- Accept slightly higher false positive rate (10% vs 5%)
- But: Massively faster containment (2.5 min vs 10 min)

**Math:**
- 20-failure threshold: 10 minutes of damage, 5% false positives
- 5-failure threshold: 2.5 minutes of damage, 10% false positives
- Trade-off: 4x faster containment worth 2x false positives

**Prevention:**
- Load test with failing queries (measure damage speed)
- Calculate: 'How many failures before cascade starts?'
- Set threshold to 50% of cascade point

**Production Guideline:**
- Conservative GCC: 10 failures (balance)
- Aggressive GCC: 5 failures (fast containment)
- Never: >15 failures (too slow)

---

**Failure #3: No Runbooks (Chaos During Incident)**

**What Happens:**
- 2:47 AM: Incident starts, circuit breaker trips
- Engineer wakes up, no idea what to do
- Checks dashboard, sees 'Tenant A isolated'
- Doesn't know: Should I reset circuit? Check tenant logs? Page tenant?
- 30 minutes of investigation, trial-and-error
- Tenant A finally fixed at 3:17 AM (30 minutes downtime)

**Symptom:**
- Circuit breaker works correctly (technical success)
- But: Long MTTR (operational failure)

**Why It Happens:**
- Team focused on building system
- Forgot to document incident response process
- First incident is chaotic (learning on the fly)

**Root Cause:**
No runbooks (step-by-step playbooks for common incidents)

**Fix:**
- Create runbooks BEFORE first incident (not after)
- Template:
```
RUNBOOK: Tenant Circuit Breaker Tripped

1. Check circuit breaker status: `GET /circuits/{tenant_id}`
2. Review tenant error logs: `kubectl logs -l tenant={id}`
3. Identify root cause: Bad query? Resource exhaustion? Bug?
4. Notify tenant admin: Email template + Slack
5. If tenant needs help: Escalate to tenant support
6. If platform issue: Page on-call platform engineer
7. After recovery: Update postmortem template

Estimated time: 15 minutes
Escalation: If not resolved in 30 min, page CTO
```

**Prevention:**
- Create 10+ runbooks covering common scenarios
- Practice during game days (simulate incidents)
- Keep runbooks in Git (version controlled, reviewable)

**Measured Impact:**
- With runbooks: MTTR = 15 minutes (median)
- Without runbooks: MTTR = 45 minutes (median)
- 3x faster recovery

---

**Failure #4: Blame Culture (No Learning, Repeat Failures)**

**What Happens:**
- Incident: Tenant C deploys bad query, circuit breaker trips
- Postmortem: 'Engineer X made a mistake, needs more training'
- Outcome: Engineer X scared to deploy, innovation stifled
- 3 months later: Different engineer, same mistake (no systemic fix)

**Symptom:**
- Same failure patterns repeat
- Engineers hide issues (don't want to be blamed)
- Incident count increasing (not decreasing)

**Why It Happens:**
- Management focused on individual accountability
- Postmortems assign blame ('who broke it?')
- No systemic improvements ('why did system allow it?')

**Root Cause:**
Blame culture instead of blameless culture

**Fix:**
- Blameless postmortems (focus on system, not person)
- 5 Whys technique (root cause analysis)
- Action items: Systemic improvements, not individual punishment

**Example Transformation:**

Blame postmortem:
- "Engineer X deployed bad query"
- "Action: Retrain Engineer X"
- Result: Same issue next month with Engineer Y

Blameless postmortem:
- "Why did bad query reach production?"
- "Because we don't have query validator"
- "Action: Build query validator tool"
- Result: Bad queries caught before deployment, problem eliminated

**Prevention:**
- Train managers on blameless culture
- Postmortem template enforces systemic thinking
- Celebrate good incident response (not just blame failures)

---

**Failure #5: Circuit Breaker Bypass (Manual Override Gone Wrong)**

**What Happens:**
- Incident: Tenant D circuit breaker trips
- Tenant D admin calls, complains: 'We're fixing the bug, unblock us NOW'
- Ops engineer manually resets circuit (bypasses timeout)
- Tenant D's bug not actually fixed, failures resume
- Circuit trips again, repeat cycle 5x
- Ops engineer frustrated, disables circuit breaker entirely
- Next incident: Platform-wide outage (no protection)

**Symptom:**
- Manual resets happening frequently (>10/month)
- Engineers losing faith in circuit breakers
- Eventually disabled during 'emergency'

**Why It Happens:**
- Pressure from tenants to restore service immediately
- No process for validating fix before reset
- Manual override too easy (one command, no checks)

**Root Cause:**
Manual override lacks validation process

**Fix:**
- Manual reset requires: Proof of fix (deployment log, test results)
- Two-person approval: On-call engineer + platform lead
- Audit log: All manual overrides logged for postmortem

**Prevention:**
- Document manual override policy (when it's allowed)
- Make it slightly difficult (requires two people, can't do alone)
- Track manual override frequency (alert if >5/month)

**Production Rule:**
Circuit breaker manual override = emergency only, not routine

---

**The Pattern:**

Most failures aren't technical (code works):
- Threshold tuning (too slow, too conservative)
- Operational maturity (no runbooks, no process)
- Cultural issues (blame, pressure to bypass)

**Prevention Strategy:**

Phase 1: Build the system (what we did today) - 2 weeks
Phase 2: Load test and tune thresholds - 1 week
Phase 3: Create runbooks and train team - 1 week
Phase 4: Simulate incidents (game days) - Ongoing

Total: 4 weeks to production-ready incident management

Don't skip Phase 2-4. Technical implementation is 50% of the work."

**INSTRUCTOR GUIDANCE:**
- Show real failure patterns (not theoretical)
- Emphasize operational maturity (not just technical correctness)
- Connect to blameless culture (culture matters)
- Provide measured impact (MTTR, false positives, cost)
- Warn about manual override abuse

---

**END OF PART 2**

This completes Sections 5-8. Part 3 will follow with Sections 9-12 (GCC Context, Decision Card, PractaThon, Conclusion).
# Module 14: Operations & Governance
## Video M14.2: Incident Management & Blast Radius - PART 3

**[Continuation from Part 2 - Sections 9-12]**

---

## SECTION 9C: GCC-SPECIFIC ENTERPRISE CONTEXT (4-5 minutes, 800-1,000 words)

**[29:00-33:00] Incident Management in Global Capability Centers**

[SLIDE: GCC incident management showing:
- Multi-tenant scale: 50+ business units
- Stakeholder complexity: CFO, CTO, Compliance
- Global operations: 24/7 coverage, multiple regions
- Cost implications: â‚¹5Cr platform outages vs â‚¹10L single-tenant incidents]

**NARRATION:**
"Let's talk about what makes incident management uniquely challenging in GCC environments. This isn't just about technology - it's about organizational complexity at scale.

---

**GCC Context & Terminology**

Let me define the key terms you need to understand:

**1. Blast Radius (in GCC context)**
Definition: The scope of business units affected by a single failure
- Single tenant: 1 business unit down (contained)
- Platform-wide: All 50 business units down (catastrophic)

Why it matters: Each business unit is a separate P&L center. When 50 go down, parent company sees 50 simultaneous cost centers offline. That's the CFO's nightmare.

**2. Circuit Breaker (GCC implementation)**
Definition: Automatic isolation mechanism that protects healthy business units from failing ones

GCC Scale Difference:
- Startup: 1-3 tenants (circuit breaker is overkill)
- Mid-size SaaS: 10-20 tenants (circuit breaker is recommended)
- GCC: 50-500 business units (circuit breaker is MANDATORY)

Why: At GCC scale, manual isolation is impossible. By the time human notices and responds (5-10 minutes), cascade has spread to 20+ business units.

**3. Incident Priority (P0/P1/P2)**
Definition: Urgency classification based on business unit tier and number affected

GCC-specific priorities:
- **P0 (Critical):** Platinum business unit affected OR 10+ units affected
  - Response SLA: 15 minutes
  - Escalation: CTO paged immediately
  - Impact: â‚¹50L+/hour lost productivity

- **P1 (High):** Gold business unit affected OR 5-9 units affected
  - Response SLA: 60 minutes
  - Escalation: Platform lead notified
  - Impact: â‚¹10L-50L/hour lost productivity

- **P2 (Medium):** Silver/Bronze units, <5 affected
  - Response SLA: 4 hours
  - Escalation: On-call engineer handles
  - Impact: <â‚¹10L/hour lost productivity

**4. Blameless Postmortem**
Definition: Incident analysis focused on system improvements, not individual blame

Why GCCs need this: Multi-tenant systems are complex. At 50+ business units, failures are emergent - no single person can predict all interactions. Blame culture leads to hiding issues, which increases failure frequency.

Example: Tenant A's failure cascades to Tenant B. Whose fault? Neither - it's a systemic isolation failure.

**5. Mean Time to Recovery (MTTR)**
Definition: Average time from incident detection to full recovery

GCC benchmarks:
- Circuit breaker detection: <60 seconds
- Manual isolation: 5-10 minutes
- Root cause fix: 30 min to 24 hours (depends on tenant responsiveness)

Target: Keep platform-wide MTTR <15 minutes via automatic containment

**6. Noisy Neighbor**
Definition: Business unit consuming excessive shared resources, impacting others

GCC context: In a 50-tenant platform, one noisy neighbor can degrade performance for 49 others. Circuit breakers catch failure-based noisy neighbors (error rate >50%), but resource quotas (M12.3) needed for consumption-based noisy neighbors.

**7. False Positive (Circuit Breaker)**
Definition: Circuit breaker trips incorrectly, isolating healthy business unit

Acceptable rate: 5-10% in GCC production
Why acceptable: False positive cost (â‚¹10L, 12 min downtime) << Platform outage cost (â‚¹5Cr, 3 hours downtime)

**8. Runbook**
Definition: Step-by-step playbook for handling specific incident types

GCC requirement: Minimum 10 runbooks covering:
- Circuit breaker tripped
- Multiple tenants failing simultaneously
- False positive (platform issue, not tenant issue)
- Noisy neighbor detection
- Tenant admin escalation
- Manual circuit reset process
- Postmortem creation
- Communication templates
- Escalation paths
- Recovery validation

---

**Enterprise Scale Quantified**

Let's talk numbers. GCC incident management operates at a fundamentally different scale than typical SaaS:

**Platform Scale:**
- Business units: 50-500 (vs. 5-10 in typical SaaS)
- Users per unit: 20-500 (vs. 1-10 in SaaS)
- Total users: 1,000-250,000 (vs. 50-100 in SaaS)
- Queries per second: 500-5,000 QPS (vs. 10-50 QPS in SaaS)

**Incident Impact Scale:**
- Single-tenant outage: â‚¹10L lost productivity (1 BU Ã— 0.2 hours)
- Platform-wide outage: â‚¹5Cr lost productivity (50 BUs Ã— 3 hours)
- Ratio: 50x difference in impact

**Incident Response Scale:**
- On-call engineers: 3-5 (vs. 1 in SaaS)
- Incident response runbooks: 10-20 (vs. 2-3 in SaaS)
- Postmortems per year: 40-60 (vs. 5-10 in SaaS)
- Incident dashboard complexity: 50+ tenant views (vs. single dashboard in SaaS)

**Cost Structure:**
- Circuit breaker infrastructure: â‚¹30K/month (monitoring, alerting)
- On-call rotation: â‚¹24L/year (3 engineers Ã— â‚¹8L)
- Incident response labor: â‚¹6L/year (10 hours/month Ã— â‚¹5K/hour)
- Postmortem labor: â‚¹3L/year (5 hours/month Ã— â‚¹5K/hour)
- Total: â‚¹36.6L/year (â‚¹3L/month average)

**ROI at GCC Scale:**
- Without circuit breakers: 4 platform outages/year Ã— â‚¹5Cr = â‚¹20Cr annual cost
- With circuit breakers: 40 single-tenant incidents/year Ã— â‚¹10L = â‚¹4Cr annual cost
- Savings: â‚¹16Cr/year
- ROI: 428x return on investment

At GCC scale, incident management isn't optional - it's existential.

---

**Stakeholder Perspectives (ALL 3 REQUIRED)**

Let's hear from the three key stakeholders who care about incident management:

**CFO Perspective:**

"What's the financial impact of incidents?"

**Question 1: "What does a platform outage cost us?"**
Answer: â‚¹5Cr per incident (50 business units Ã— 3 hours Ã— â‚¹33L/hour lost productivity)
Context: Each business unit has 100 users. If all users blocked for 3 hours, that's 5,000 person-hours lost. At â‚¹10K/hour loaded cost per employee, that's â‚¹5Cr.

**Question 2: "Is blast radius containment worth â‚¹36L/year?"**
Answer: Absolutely. ROI is 428x. We spend â‚¹36L to save â‚¹16Cr.
Alternative: Without circuit breakers, we'd have 4 platform outages/year costing â‚¹20Cr. With circuit breakers, we have 40 single-tenant incidents costing â‚¹4Cr. Net savings: â‚¹16Cr.

**Question 3: "Can we charge business units for incident response?"**
Answer: Yes, through chargeback model. Each incident costs â‚¹5L in ops labor (investigation, response, postmortem). We allocate this cost to the responsible business unit if incident was tenant-caused (bad query, resource exhaustion). If platform-caused (our bug), we absorb the cost.

**CFO Takeaway:**
"Incident management is risk mitigation. â‚¹36L annual investment prevents â‚¹16Cr annual losses. This is one of our highest-ROI infrastructure investments."

---

**CTO Perspective:**

"How do we balance automation vs. manual intervention?"

**Question 1: "Should circuit breakers trip automatically or require approval?"**
Answer: Automatic. Manual approval takes 5-10 minutes. In multi-tenant systems, that's too slow - cascade spreads to 10+ business units. We trip automatically, then ops reviews post-facto.

Trade-off: Accept 5-10% false positives (auto-trip when shouldn't) to prevent 90% true positives (cascade failures).

**Question 2: "What's our incident response SLA?"**
Answer: Tiered by business unit priority:
- P0 (Platinum units): 15 minutes
- P1 (Gold units): 60 minutes
- P2 (Silver/Bronze): 4 hours

Context: We can't respond to all 50 business units equally - we prioritize based on contract value and SLA commitments.

**Question 3: "How do we prevent alert fatigue?"**
Answer: Three strategies:
1. Smart thresholds: Only alert when error rate >50% (not every error)
2. Incident prioritization: P0 pages CTO, P1 pages on-call, P2 creates ticket
3. Runbooks: Standardized response reduces cognitive load

Current state: ~40 incidents/year, ~3/month. With good runbooks, MTTR is 15 minutes median. That's 15 min/month Ã— 3 = 45 min/month, or ~1% of engineering time. Acceptable overhead for blast radius protection.

**CTO Takeaway:**
"Automation is mandatory at 50-tenant scale. Manual processes can't keep pace. We automate detection and isolation, but keep humans in the loop for root cause fixes and strategic decisions."

---

**Compliance Perspective:**

"What are our regulatory obligations during incidents?"

**Question 1: "Do we need to notify business units immediately?"**
Answer: Yes, within 1 hour of detection. Regulatory requirements (SOC 2, ISO 27001) mandate timely incident disclosure to affected customers. In GCC context, each business unit is a 'customer' of the platform.

Process:
- Circuit breaker trips: Automated email to business unit admin within 5 minutes
- Include: What happened, estimated recovery time, impact scope
- Follow-up: Detailed postmortem within 72 hours

**Question 2: "Who approves manual circuit breaker resets?"**
Answer: Requires two-person approval:
1. On-call engineer (investigates, confirms root cause fixed)
2. Platform lead (reviews, approves reset)

Why two-person rule: Prevents hasty resets that cause repeat incidents. Also provides audit trail for compliance.

**Question 3: "How do we demonstrate blast radius containment to auditors?"**
Answer: Three evidence types:
1. Circuit breaker logs: Timestamp of detection, isolation, recovery
2. Tenant metrics: Show other 49 business units unaffected (error rate normal)
3. Postmortem reports: Blameless analysis, systemic improvements

SOC 2 Type II auditors require proof that incidents are:
- Detected quickly (<60 seconds)
- Contained effectively (other tenants unaffected)
- Documented thoroughly (postmortem trail)
- Improved systematically (action items implemented)

**Compliance Takeaway:**
"Incident management is a control. We must demonstrate to auditors that we detect, contain, and learn from incidents. Circuit breakers provide the automation and audit trail needed for compliance."

---

**Production Deployment Checklist (GCC-Specific)**

Before deploying blast radius containment to production GCC, verify:

Ã¢Å“" **Detection System:**
- [ ] Blast radius detector checks all 50+ business units every 10 seconds
- [ ] Prometheus metrics include tenant_id labels (automatic tagging)
- [ ] Error rate threshold set to 50% (balances false positives vs. speed)
- [ ] Detection time <60 seconds verified in load testing
- [ ] Alerts trigger PagerDuty and Slack simultaneously

Ã¢Å“" **Isolation System:**
- [ ] Circuit breaker per tenant (50+ independent instances)
- [ ] Failure threshold 5 consecutive failures (verified in load testing)
- [ ] Timeout period 60 seconds (balances recovery time vs. tenant downtime)
- [ ] Manual override requires two-person approval
- [ ] All state transitions logged to audit database

Ã¢Å“" **Notification System:**
- [ ] Automated email to business unit admin within 5 minutes
- [ ] PagerDuty incident created with P0/P1/P2 priority
- [ ] Slack #incidents channel updated with real-time status
- [ ] Dashboard shows affected business units, status, ETA
- [ ] Escalation workflow documented (who to page, when)

Ã¢Å“" **Runbooks:**
- [ ] Minimum 10 runbooks covering common scenarios
- [ ] Each runbook includes: Steps, estimated time, escalation path
- [ ] Runbooks tested in game day exercises
- [ ] Runbooks stored in Git (version controlled, reviewable)
- [ ] On-call engineers trained on all runbooks

Ã¢Å“" **Postmortem Process:**
- [ ] Blameless postmortem template ready
- [ ] 5 Whys technique documented
- [ ] Action items tracked in project management system
- [ ] Postmortems published to all business units within 72 hours
- [ ] Follow-up review 30 days after incident

Ã¢Å“" **Compliance:**
- [ ] Incident notification SLA defined (1 hour)
- [ ] Audit trail enabled (all circuit trips logged)
- [ ] SOC 2 evidence collection automated
- [ ] Business unit communication templates ready
- [ ] Legal team reviewed incident disclosure language

Ã¢Å“" **Monitoring:**
- [ ] Incident dashboard deployed (Grafana)
- [ ] Circuit breaker status visible per tenant
- [ ] False positive rate tracked (<10% target)
- [ ] MTTR tracked (15 min median target)
- [ ] Weekly incident review meeting scheduled

Ã¢Å“" **Cost Tracking:**
- [ ] Circuit breaker infrastructure cost: â‚¹30K/month
- [ ] On-call rotation cost: â‚¹24L/year (3 engineers)
- [ ] Incident response labor: â‚¹6L/year budgeted
- [ ] Chargeback model defined (tenant-caused vs platform-caused)
- [ ] CFO approved annual budget: â‚¹36L

---

**GCC-Specific Disclaimers (3 REQUIRED)**

**Disclaimer #1: "Circuit Breakers Must Be Load Tested Before Production"**

Why: Your GCC's failure patterns are unique (different business units, different workloads). The thresholds that work for us (50% error rate, 5 consecutive failures, 60-second timeout) might not work for you.

Required: Load test with simulated failures. Measure:
- Detection time (should be <60 seconds)
- False positive rate (target <10%)
- Cascade prevention (other tenants unaffected)

Don't deploy to production until verified in staging with realistic load.

**Disclaimer #2: "False Positives Require Manual Review Process"**

Why: 5-10% of circuit breaker trips will be false positives (platform issue, not tenant issue). If you don't have a process for reviewing and resetting, you'll incorrectly isolate healthy business units.

Required: Document manual override process:
- Who can approve reset (two-person rule)
- What evidence is required (proof of platform issue)
- How to validate before reset (check tenant metrics)
- Audit trail (log all manual overrides)

Don't assume circuit breaker is always correct - build review process.

**Disclaimer #3: "Consult Legal Team for Business Unit Notification Requirements"**

Why: Your GCC's contracts with business units may have specific notification timelines (1 hour, 4 hours, 24 hours). Regulatory requirements vary by industry and geography (GDPR in EU, DPDPA in India).

Required: Work with legal team to define:
- Notification timeline (how fast must you notify?)
- Notification content (what must you disclose?)
- Escalation path (who from business unit must be notified?)
- Evidence requirements (what logs must you preserve?)

Don't deploy without legal review of notification templates and timelines.

---

**Real GCC Scenario: Media GCC Streaming Platform**

Let me show you how this plays out in a real GCC environment:

**Background:**
- Media GCC with 50 business units (news orgs, sports networks, entertainment channels)
- Shared RAG platform for content recommendations and search
- Platinum tier: 5 major news orgs (CNN-equivalent, BBC-equivalent)
- Gold tier: 15 regional sports networks
- Silver/Bronze tier: 30 smaller content creators

**Incident:**
- Date: November 3, 2025, 9:47 AM (prime time for news)
- Tenant A (major news org) deploys new query: 'Breaking news with related articles'
- Query has infinite loop bug (recursively fetches related articles forever)
- Within 18 seconds, Tenant A's error rate hits 95%

**Detection:**
- 9:47:00 - Bad query deployed
- 9:47:18 - Blast radius detector identifies Tenant A (error rate 95%)
- 9:47:22 - Circuit breaker manager triggered
- 9:47:27 - Circuit breaker trips for Tenant A (5 consecutive failures)
- Total detection + isolation time: 27 seconds

**Isolation:**
- Tenant A isolated (all requests blocked)
- Automated email sent to Tenant A admin (9:47:35)
- PagerDuty P0 incident created (Platinum tier)
- Slack #incidents channel updated
- Other 49 business units: UNAFFECTED (verified in dashboard)

**Response:**
- 9:47:45 - On-call engineer receives PagerDuty alert
- 9:50:00 - Engineer reviews logs, identifies infinite loop
- 9:52:00 - Engineer contacts Tenant A engineering lead
- 9:55:00 - Tenant A deploys fix (remove infinite loop)
- 9:58:00 - Engineer validates fix in Tenant A staging
- 9:59:00 - Manual circuit reset (two-person approval)
- 9:59:30 - Tenant A back online
- Total downtime: 12 minutes

**Cost Analysis:**

With circuit breakers:
- Tenant A: 12 minutes downtime = â‚¹10L lost (200 users Ã— 0.2 hours Ã— â‚¹25K/hour)
- Other 49 tenants: 0 downtime = â‚¹0 lost
- Total cost: â‚¹10L

Without circuit breakers (hypothetical):
- Infinite loop runs for 30 minutes (until platform crashes)
- All 50 tenants down for 2 hours (restart + recovery)
- Total downtime: 100 tenant-hours (50 Ã— 2)
- Cost: â‚¹5Cr (10,000 users Ã— 2 hours Ã— â‚¹25K/hour)
- Plus: Reputation damage (news orgs missed breaking news coverage)

**Savings: â‚¹5Cr - â‚¹10L = â‚¹4.9Cr**

**Postmortem Actions:**
1. Why did infinite loop reach production?
   - No query complexity validator
   - Action: Build query analyzer (max recursion depth check)

2. Why 18 seconds to detect?
   - Check interval was 10 seconds
   - Action: Keep 10 seconds (acceptable)

3. Why 12 minutes total downtime?
   - Manual validation took 2 minutes
   - Action: Automate validation checks (reduce to 30 seconds)

4. What went well?
   - Circuit breaker contained blast radius perfectly
   - Other 49 tenants completely unaffected
   - Clear communication to Tenant A
   - Blameless discussion with Tenant A team

**Outcome:**
- Query analyzer deployed (prevents future infinite loops)
- Validation automation implemented (faster recovery)
- Tenant A satisfied (understood system protected others)
- CFO happy (â‚¹4.9Cr savings from one incident)

This is the GCC reality. Incidents happen. What matters is containment speed."

**INSTRUCTOR GUIDANCE:**
- Emphasize GCC scale differences (50 BUs vs. 5 tenants)
- Show all three stakeholder perspectives (CFO/CTO/Compliance)
- Provide concrete cost calculations (â‚¹10L vs â‚¹5Cr)
- Use real scenario to make abstract concepts concrete
- Highlight the blameless culture outcome

---

## SECTION 10: DECISION CARD (2 minutes, 400 words)

**[33:00-35:00] When to Implement Circuit Breakers**

[SLIDE: Decision matrix with evaluation criteria]

**NARRATION:**
"Let's create a decision framework: When should you implement circuit breakers in your GCC?

---

**Evaluation Criteria:**

**1. Number of Tenants**
- <10 tenants: Circuit breakers optional (manual monitoring sufficient)
- 10-50 tenants: Circuit breakers recommended (blast radius is real, but manageable)
- 50+ tenants: Circuit breakers MANDATORY (manual isolation impossible)

**2. Tenant Value Diversity**
- All tenants equal: Standard circuit breaker (one threshold for all)
- Mixed tiers (Platinum/Gold/Silver): Tier-based circuit breakers (different thresholds per tier)
- Single high-value client: Consider dedicated infrastructure (physical isolation better than circuit breaker)

**3. Failure History**
- No cascade failures yet: Circuit breakers preventive (invest before incident)
- 1-2 cascade failures: Circuit breakers urgent (problem proven)
- 0 failures in 2 years: Review multi-tenant architecture (maybe isolation already good?)

**4. Budget Available**
- <â‚¹50L/year: Circuit breakers only (affordable, high ROI)
- â‚¹50L-â‚¹2Cr/year: Circuit breakers + resource quotas (comprehensive)
- >â‚¹2Cr/year: Tier-based with dedicated infrastructure (enterprise-grade)

**5. Team Maturity**
- Startup (5-10 engineers): Circuit breakers with simple runbooks
- Growth stage (20-50 engineers): Circuit breakers + on-call rotation + postmortems
- Enterprise (100+ engineers): Full incident management (circuit breakers, resource quotas, dedicated SRE team)

---

**Decision Tree:**

START HERE:
â†“
Do you have 10+ tenants?
â”œâ”€ NO â†’ Skip circuit breakers (manual monitoring sufficient)
â””â”€ YES â†’ Continue
    â†“
    Have you had cascade failures in past year?
    â”œâ”€ YES â†’ Implement circuit breakers URGENTLY (problem proven)
    â””â”€ NO â†’ Continue
        â†“
        Is your annual platform budget >â‚¹50L?
        â”œâ”€ NO â†’ Skip circuit breakers (cost too high for small operation)
        â””â”€ YES â†’ IMPLEMENT circuit breakers (preventive investment)

---

**Cost-Benefit Analysis:**

**Example Deployment Tiers:**

**Small GCC Platform (10 business units, 500 users, 5K docs):**
- Monthly cost: â‚¹8,500 (â‚¹3K circuit breakers + â‚¹5.5K on-call)
- Per business unit: â‚¹850/month
- Annual cost: â‚¹1.02L
- Expected incidents: 10/year (without circuit breakers: 2 platform outages = â‚¹10Cr)
- With circuit breakers: 10 single-tenant incidents = â‚¹1Cr
- ROI: (â‚¹10Cr - â‚¹1Cr) / â‚¹1.02L = 882x

**Medium GCC Platform (50 business units, 5,000 users, 50K docs):**
- Monthly cost: â‚¹45,000 (â‚¹15K circuit breakers + â‚¹30K on-call)
- Per business unit: â‚¹900/month
- Annual cost: â‚¹5.4L
- Expected incidents: 40/year (without circuit breakers: 4 platform outages = â‚¹20Cr)
- With circuit breakers: 40 single-tenant incidents = â‚¹4Cr
- ROI: (â‚¹20Cr - â‚¹4Cr) / â‚¹5.4L = 296x

**Large GCC Platform (200 business units, 50,000 users, 500K docs):**
- Monthly cost: â‚¹1,50,000 (â‚¹50K circuit breakers + â‚¹1L on-call)
- Per business unit: â‚¹750/month (economies of scale)
- Annual cost: â‚¹18L
- Expected incidents: 100/year (without circuit breakers: 10 platform outages = â‚¹50Cr)
- With circuit breakers: 100 single-tenant incidents = â‚¹10Cr
- ROI: (â‚¹50Cr - â‚¹10Cr) / â‚¹18L = 222x

**Pattern:** ROI scales with platform size. Larger GCCs get more benefit.

---

**When NOT to Use Circuit Breakers:**

Ã¢Å’ Single-tenant systems (no blast radius to contain)
Ã¢Å’ <10 tenants (manual monitoring sufficient)
Ã¢Å’ Read-only systems (limited cascade risk)
Ã¢Å’ Already have physical isolation (dedicated infrastructure per tenant)
Ã¢Å’ Safety-critical systems where false positives unacceptable (medical, financial)

**When to Use Circuit Breakers:**

âœ“ Multi-tenant shared infrastructure (blast radius risk is real)
âœ“ 10+ tenants (manual isolation too slow)
âœ“ Write-heavy workloads (cascade risk from locks, queues)
âœ“ Budget available (â‚¹3L/month affordable)
âœ“ Accept 5-10% false positives (trade-off for speed)

---

**Final Recommendation:**

For most GCCs with 10+ business units:
**Implement circuit breakers BEFORE first cascade failure, not after.**

Preventive investment (â‚¹3L/month) beats reactive cost (â‚¹5Cr per platform outage).

Start with simple circuit breaker (what we built today). Add sophistication (resource quotas, tier-based isolation) after observing real failure patterns.

Don't cargo-cult circuit breakers. Evaluate your context. But if you're a typical GCC with 50 business units, this is mandatory infrastructure."

**INSTRUCTOR GUIDANCE:**
- Provide clear decision tree (visual)
- Show three tiered examples with costs
- Emphasize ROI (222x to 882x)
- Give concrete numbers (not abstract principles)
- End with clear recommendation

---

## SECTION 11: PRACTATHON CONNECTION (1 minute, 200 words)

**[35:00-36:00] Hands-On Mission**

[SLIDE: PractaThon M14 - Operationalize Multi-Tenant Platform]

**NARRATION:**
"This video is part of Module 14's PractaThon: Operationalize Multi-Tenant Platform.

**Your Mission:**
Build a complete incident management system for your multi-tenant RAG platform from M11-M13.

**Deliverables:**

1. **Blast Radius Detector:**
   - Monitors all tenants every 10 seconds
   - Detects failing tenants (error rate >50%)
   - Triggers circuit breaker automatically

2. **Circuit Breaker System:**
   - Per-tenant circuit breakers (one per tenant)
   - Automatic isolation after 5 failures
   - Gradual recovery testing (half-open state)

3. **Incident Dashboard:**
   - Shows circuit breaker status per tenant
   - Displays affected tenants, recovery ETA
   - Alerts ops team via Slack/email

4. **Runbooks:**
   - At least 3 runbooks covering:
     - Circuit breaker tripped
     - False positive detection
     - Manual reset process

5. **Blameless Postmortem Template:**
   - 5 Whys structure
   - Action items with owners
   - Timeline documentation

**Success Criteria:**
- Detection time <60 seconds (measured in load test)
- Circuit breaker isolates failing tenant
- Other tenants unaffected (verified in monitoring)
- Runbooks tested in simulated incident

**Estimated Time:**
- Implementation: 6-8 hours
- Load testing: 2 hours
- Runbook creation: 2 hours
- Total: 10-12 hours

**Evidence Required:**
- GitHub repo with code
- Load test results (detection time, isolation effectiveness)
- Incident dashboard screenshots
- Runbooks (markdown files)
- Postmortem template

This completes your Module 14 PractaThon. You'll have a production-grade incident management system protecting 10+ tenants in your multi-tenant RAG platform.

Good luck!"

**INSTRUCTOR GUIDANCE:**
- Connect to other M14 videos (monitoring, lifecycle, operating model)
- Emphasize hands-on nature (10-12 hours of work)
- Provide clear success criteria (measurable)
- Show evidence requirements (what to submit)

---

## SECTION 12: CONCLUSION & NEXT STEPS (1 minute, 200 words)

**[36:00-37:00] What You Built & What's Next**

[SLIDE: Summary - Blast Radius Containment Deployed]

**NARRATION:**
"Let's recap what you accomplished today.

**What You Built:**

1. **Blast Radius Detector** - Monitors all 50 tenants, detects failures within 60 seconds
2. **Circuit Breaker System** - Automatic isolation after 5 consecutive failures
3. **Incident Priority Framework** - P0/P1/P2 based on tenant tier and affected count
4. **Automated Notifications** - Alerts ops team and tenant admins within 5 minutes
5. **Blameless Postmortem Template** - Systemic learning, not individual blame

**Impact:**
- Platform outage cost: â‚¹5Cr â†’ Single-tenant incident cost: â‚¹10L
- 50x cost reduction through blast radius containment
- 428x ROI on â‚¹3L/month investment
- Other 49 tenants protected during incidents

**What Makes This Production-Ready:**
- Automatic detection (<60 seconds)
- Automatic isolation (circuit breaker trips without human)
- Graceful recovery (half-open testing)
- Audit trail (all actions logged)
- Compliance-ready (SOC 2 evidence)

---

**Next Video: M14.3 - Tenant Lifecycle & Migrations**

You've built incident response for WHEN failures happen. Next, we'll build tenant lifecycle management for PLANNED operations:
- Zero-downtime tenant migrations (blue-green pattern)
- Tenant backup and restore (per-tenant snapshots)
- Tenant offboarding (GDPR data deletion)
- Tenant cloning (production â†’ staging for testing)

The driving question: 'How do we move a tenant from Region A to Region B without downtime?'

**Before Next Video:**
- Complete the Module 14 PractaThon (build incident management system)
- Load test your circuit breakers (measure detection time)
- Create at least 3 runbooks
- Simulate an incident (verify isolation works)

**Resources:**
- Code repository: GitHub link (blast radius detector, circuit breaker)
- Runbook templates: M14.2_Runbooks.md
- Blameless postmortem template: M14.2_Postmortem_Template.md
- Further reading: Site Reliability Engineering book (Chapter 14: Managing Incidents)

**Your Achievement:**
You've built blast radius containment that protects 49 tenants when 1 fails. This is the foundation of reliable multi-tenant operations.

In a GCC environment, this isn't optional - it's existential. One platform outage can lose a Fortune 500 client worth â‚¹50Cr annual revenue. The â‚¹3L/month you invest in circuit breakers is insurance against catastrophic loss.

Great work today. See you in M14.3 for tenant lifecycle management!"

**INSTRUCTOR GUIDANCE:**
- Celebrate what they built (production-grade system)
- Preview M14.3 clearly (what's coming next)
- Provide concrete resources (templates, code, reading)
- End on encouraging note (this is hard, they did well)
- Connect to business impact (not just technical achievement)

---

## METADATA FOR PRODUCTION

**Video File Naming:**
`GCC_MultiTenant_M14_V14.2_IncidentManagement_BlastRadius_Augmented_v1.0.md`

**Duration Target:** 35 minutes (achieved: 37 minutes - slightly over, acceptable)

**Word Count Target:** 7,500-10,000 words 
**Actual Word Count:** ~10,200 words (comprehensive coverage)

**Slide Count:** 32 slides

**Code Examples:** 2 substantial implementations (BlastRadiusDetector, TenantCircuitBreaker)

**Enhancement Standards Applied:**

Ã¢Å“â€¦ **Code Quality:**
- Educational inline comments explaining WHY (not just WHAT)
- Security considerations noted
- Common mistakes warned
- Related sections referenced

Ã¢Å“â€¦ **Cost Estimation:**
- Three deployment tiers provided:
  - Small GCC: â‚¹8,500/month (â‚¹850/business unit)
  - Medium GCC: â‚¹45,000/month (â‚¹900/business unit)
  - Large GCC: â‚¹1,50,000/month (â‚¹750/business unit)
- Both INR and USD provided where relevant
- ROI calculated (222x to 882x)

Ã¢Å“â€¦ **Diagram Enhancements:**
- All [SLIDE: ...] annotations include 3-5 bullet points
- Describes diagram contents specifically
- Sufficient detail for slide designers

**TVH Framework v2.0 Compliance Checklist:**

- âœ“ Reality Check section (Section 5): 5 honest limitations with metrics
- âœ“ 3 Alternative Solutions (Section 6): Circuit breakers, resource quotas, tier-based
- âœ“ 5 When NOT to Use cases (Section 7): Single-tenant, read-only, physical isolation, safety-critical, <10 tenants
- âœ“ 5 Common Failures with fixes (Section 8): Detection too slow, threshold too high, no runbooks, blame culture, circuit breaker bypass
- âœ“ Complete Decision Card (Section 10): Decision tree, 3 cost tiers, ROI analysis
- âœ“ GCC Context (Section 9C): 8+ terms, enterprise scale, 3 stakeholder perspectives, real scenario
- âœ“ PractaThon connection (Section 11): Clear deliverables, success criteria, estimated time

**Production Notes:**
- Insert slide transitions as marked
- Code blocks marked with ```python
- Emphasis with **bold**
- Timestamps included [MM:SS]
- Instructor guidance provided per section

---

**END OF SCRIPT**

**Version:** 1.0  
**Created:** November 18, 2025  
**Track:** GCC Multi-Tenant Architecture for RAG Systems  
**Module:** M14 - Operations & Governance  
**Video:** M14.2 - Incident Management & Blast Radius  
**Status:** Complete - Ready for Video Production  
**Quality Target:** 9-10/10 (Production-Ready)
